%!TEX root = ../thesis.tex
%*******************************************************************************
%*********************************** Fifth Chapter *****************************
%*******************************************************************************

\ifpdf
     \graphicspath{{/Users/luyolomagangane/Documents/Academics/Figures/Chapter5/}}
\else
    \graphicspath{{Chapter1/Figs/Vector/}{Chapter1/Figs/}}
\fi

\chapter{Results and Analysis}  %Title of the Fifth Chapter

\section{Nonlinear Tensor Factorisation}
% Background on 3 hypotheses that were tested
\subsubsection{Overview}
This chapter explores three hypotheses explored for link prediction using knowledge graph nonlinear tensor factroisation, namely the application of best practise deep learning techniques to recursive neural tensor networks, relational filter regularisation using dropout in hyper convolutional neural tensor networks, and finally the application of pretrained word embeddings in hyper convolutional neural tensor factorisation. \newline 

Tensor factorisation is the use of high dimensional tensors for relational modeling ~\citep{Nickel_2016}. Statistical relational learning has traditionally relied on linear modelling techniques in order to constrain parameterisation for computational efficiency at the expense of model expressiveness i.e. models that suffer from high bias. Recently nonlinear approaches have been proposed to overcome the bias problem and each propose hypothesis that aim to take advantage of their distinct architecture. The first nonlinear model to gain surpass state-of-the-art tensor factorisation models, was the recursive neural tensor network \cite{refefence}. There are two hypotheses proposed that were proposed in using this model architecture for link prediction, namely that recursive architectures are useful in part of speech tagging \cite{reference}, as the perform a combinatorial analysis of word sequences in sentences, instead of the conventional linear sequence used in part of speech tagging \cite{reference}. The second hypothesis is that pre-trained word embeddings offer superior performance to randomly generated entity vectors which were commonly used at the time for tensor factorisation \cite{reference}. \newline
The second hypothesis we explored is the use of convolutional hyper networks with relational factorisation for link prediction \cite{reference}. This research is the current state-of-that-art latent feature modelling technique for link prediction \cite{reference}. There are also two hypotheses explored in this research - the first is that convolutional filters can be used explicitly for relational factorisation \cite{reference}, the second is that the hyper network architecture is useful in generating the relational filters due to increased parameterisation using a hyper linear layer \cite{reference}. \newline
The final hypothesis we explored is the use of contextual entity representations as input to convolutional hyper networks. State-of-the-art natural language modelling techniques makes use contextual word representations \cite{Elmo, Bert} instead of static word embeddings \cite{fasttext, GloVe, Word2Vec}. Contextual word embeddings are represented as distinct subject entity representations, and distinct object entity representations. \newline
The following chapter presents the results and analysis from the experiments performed to validate these hypotheses.

\section{Recursive Neural Tensor Networks}
\subsubsection{Model Summary} 
Tensor factorisation has been a popular approach applied to statistical relational learning (SRL) \cite{reference, reference, reference}. In order to improve model expressiveness, a Recursive Neural Tensor Network architecture was proposed \cite{reference}. This model is inspired by recursive neural networks \cite{reference}. Recursive neural networks have been applied to natural language processing tasks, particularly part of speech tagging \cite{reference}. Traditionally, recurrent neural networks have been applied to this task \cite{reference}. Recurrent neural networks are based on the Markov assumption \cite{reference}, where the future states of the process depends only on the present state, not on the sequence of events that preceded it \cite{reference}. The state is constructed using sequential input within a time step window, for example, a sentence will contain a sequence of words, and the window will be the last word in the sentence and the four previous words before an anchor word. The state consists of the four previous words before the anchor word, and along with the anchor word, can be used to predict the next word in the sequence.\newline
Instead of producing a state using a sequence of previous words, recursive neural networks generate the state using a combinatorial tree of the word sequence. The number of potential combinations are dependent on the length of the sequence, for example a three word sequence has three combinations, a four word sequence has six combinations and  a five word sequence has ten combinations. The combinations which produce the smallest magnitudes are then filtered out from the network, and remaining combinations are then recombined as new combinations, This filtering and recombination process is then continued until a next word prediction is made. \newline
Recursive neural networks are combined with tensor factorisation to produce a more expressive model for link prediction. The subject and object entities are concatenated and then an matrix multiplication is taken with the resultant vector before a bias is added. \newline
\subsubsection{Contrastive Max Margin Loss}
The contrastive max-magrin loss \cite{reference} is used to train the RTNT model. The input consists of an entity-relational pair, where the entity is a subject entity. An object entity is presented as a target to complete the triple. A non-related object entity is presented is then presented as a corrupt object entity. Fact score is then computed for the target triple, as well as the corrupt triple as logits, and the difference between the two logits is the contrast between the true and false facts. The task is for the model is then to produce a higher fact score for the true triple than the false triple. If the model gets it wrong, then loss is generated and back propagated through the network to update model parameters. \newline

\subsubsection{Experimental Setup} 

We use the following benchmark knowledge graphs: Wordnet - a lexical database for English, it is a taxonomy with hypernyms (is-a) relationships, and synonym sets. \newline
Freebase - a large collaborative knowledge base consisting of data about the world composed mainly by its community members. It was an online collection of structured data harvested from many sources, including user-submitted wiki contributions. \newline
Visualisations of the respective knowledge graphs are presented below:

\begin{figure}[H]
  	\caption{Wordnet Entities and Relations Graphplot}
   	\centering
    	\includegraphics[width=\textwidth]{Wordnet}
\end{figure}

\begin{figure}[H]
  	\caption{Freebase Entity and Relations Graphplot}
   	\centering
    	\includegraphics[width=\textwidth]{Freebase}
\end{figure}


We used the Tensorflow framework to develop our model.  This model is built on top of the Neural Tensor model introduced by (Socher, Chen, Manning and Ng 2013) ~\citep{NIPS2013_5028} and implemented in Tensorflow by (Doss, LeNail and Liu 2015)  ~\citep{Doss2015}. Randomly initialised entity and relational embeddings are used to initialise model training. These embeddings are dynamically adjusted during the training process to generate latent representations specific to the knowledge domain. Property counts for the respective knowledge graphs are presented below:

\begin{figure}[H]
	\parbox{.5\linewidth}{
   		\caption{Wordnet Property Barplot}
   		\centering
    		\includegraphics[width=0.45\textwidth]{Wordnet_Counts}
		}
	\hfill
	\parbox{.5\linewidth}{
		\caption{Freebase Property Barplot}
   		\centering
    		\includegraphics[width=0.45\textwidth]{Freebase_Counts}
		}
\end{figure}


\begin{table}[H]
	\parbox{.5\linewidth}{
		\caption{Wordnet Property Counts}
		\centering
		\begin{tabular}{lllllllllll}
  			\textbf{Property} & \textbf{Count}  \\
  			\hline
  			Entities & 38,696  \\
  			Relations & 11  \\
  			Triples & 136,611  \\
		\end{tabular}
		}
	\hfill
	\parbox{.5\linewidth}{
		\caption{Freebase Property Counts}
		\centering
		\begin{tabular}{lllllllllll}
  			\textbf{Property} & \textbf{Count}  \\
  			\hline
  			Entities & 75,043   \\
  			Relations & 13  \\
  			Triples & 375,499  \\
		\end{tabular}
		}
\end{table}


Summary statistics of the respective knowledge graphs Resource Description Framework (RDF) decomposition - subject, predicate, object - are presented below:

% Predicate

\begin{figure}[H]
	\parbox{.5\linewidth}{
   		\caption{Wordnet Predicate Barplot}
   		\centering
    		\includegraphics[width=0.45\textwidth, height=0.2\textheight]{Wordnet_Predicate_Counts}
		}
	\hfill
	\parbox{.5\linewidth}{
		\caption{Freebase Predicate Barplot}
   		\centering
		\includegraphics[width=0.45\textwidth, height=0.2\textheight]{Freebase_Predicate_Counts}
		}
\end{figure}

\begin{table}[H]
	\parbox{.5\linewidth}{
		\caption{Wordnet Predicate Statistics}
		\centering
		\begin{tabular}{lllllllllll}
  			\textbf{Statistic} & \textbf{Value}  \\
  			\hline
			Count & 11 \\
			Max & 43,312  \\
			Min & 1,229  \\
  			Median & 7,705  \\
  			IQR & 7,257.5  \\
		\end{tabular}
		}
	\hfill
	\parbox{.5\linewidth}{
		\caption{Freebase Predicate Statistics}
		\centering
		\begin{tabular}{lllllllllll}
  			\textbf{Statistic} & \textbf{Value}  \\
  			\hline
			Count & 13 \\
			Max & 73,897  \\
			Min & 4, 464  \\
  			Median & 21,149  \\
  			IQR & 34,033  \\
		\end{tabular}
		}
\end{table}

% Subject

\begin{figure}[H]
	\parbox{.5\linewidth}{
   		\caption{Wordnet Subject Barplot}
   		\centering
    		\includegraphics[width=0.45\textwidth, height=0.2\textheight]{Wordnet_Subject_Counts}
		}
	\hfill
	\parbox{.5\linewidth}{
		\caption{Freebase Subject Barplot}
   		\centering
		\includegraphics[width=0.45\textwidth, height=0.2\textheight]{Freebase_Subject_Counts}
		}
\end{figure}


\begin{table}[H]
	\parbox{.5\linewidth}{
		\caption{Wordnet Subject Statistics}
		\centering
		\begin{tabular}{lllllllllll}
  			\textbf{Statistic} & \textbf{Value}  \\
  			\hline
			Count & 32,720 \\
			Max & 619 \\
			Min & 1 \\
  			Median & 2 \\
  			IQR & 2 \\
		\end{tabular}
		}
	\hfill
	\parbox{.5\linewidth}{
		\caption{Freebase Subject Statistics}
		\centering
		\begin{tabular}{lllllllllll}
  			\textbf{Statistic} & \textbf{Value}  \\
  			\hline
			Count & 67,393 \\
			Max & 37 \\
			Min & 1 \\
  			Median & 5 \\
  			IQR & 4 \\
		\end{tabular}
		}
\end{table}

% Object

\begin{figure}[H]
	\parbox{.5\linewidth}{
   		\caption{Wordnet Object Barplot}
   		\centering
    		\includegraphics[width=0.45\textwidth, height=0.2\textheight]{Wordnet_Object_Counts}
		}
	\hfill
	\parbox{.5\linewidth}{
		\caption{Freebase Object Barplot}
   		\centering
		\includegraphics[width=0.45\textwidth, height=0.2\textheight]{Freebase_Object_Counts}
		}
\end{figure}


\begin{table}[H]
	\parbox{.5\linewidth}{
		\caption{Wordnet Object Statistics}
		\centering
		\begin{tabular}{lllllllllll}
  			\textbf{Statistic} & \textbf{Value}  \\
  			\hline
			Count & 33,011 \\
			Max & 537 \\
			Min & 1 \\
  			Median & 3 \\
  			IQR & 2 \\
		\end{tabular}
		}
	\hfill
	\parbox{.5\linewidth}{
		\caption{Freebase Object Statistics}
		\centering
		\begin{tabular}{lllllllllll}
  			\textbf{Statistic} & \textbf{Value}  \\
  			\hline
			Count & 15,342 \\
			Max & 59,663 \\
			Min & 1 \\
  			Median & 3 \\
  			IQR & 6 \\
		\end{tabular}
		}
\end{table}

For Wordnet, it can be seen that relations are skewed toward "has instance" 43,312 occurrences, and "type of" 36,659 occurrences. We would expect poor performance from the model for out-of-sample data containing
containing relations that are not "has instance" or "type of". Freebase relations are somewhat more uniform, however four relations have occurrences under 10,000. We would expect reasonable performance across all relations for this knowledge graph. \newline
Wordnet and Freebae subjects are somewhat uniform, although the median number of occurrences is 2 and 5 respectively, with an IQR of 2 and 4 respectively. This are thus sparse distributions of data points, and the model will need to rely on 
similarity of subject features in order to produce sensible inference. \newline
Wordnet object occurrences are somewhat uniform. Freebase object occurrences are skewed, with a single object, "male" occurring 59,663 times, representing 15,88\% of facts. This is in comparison to a median object occurrence of 3 and an interquartile range of 6.
We would expect poor performance from a Freebase link prediction model given this distribution of objects. \newline

% Best Practise Deep Learning Techniques

\subsubsection{Hypothesis 1: \newline 
Recursive Neural Tensor Networks with Best Practise Deep Learning Techniques}
Model hyperparameter optimisation is implemented using random grid search. The the model was trained on a MacBook Pro 2015 with 8 cores, 16GB RAM, and 512GB SSD. \newline
We evaluate the model by ranking the accuracy scores of the predicted triples for the respective datasets. Code to reproduce: https://github.com/xhosaBoy/deep-knowledge-modelling

\subsubsection{Bias Variance Trade Off}
Machine learning models can suffer from overfitting \cite{reference} - when training has progressed too long, and the model parameters have been optimised for prediction on the training set. This leads to poor generalisation  and reveals itself in the form of increasing training accuracy, and decreasing validation accuracy. In order to overcome this problem, early stopping is often used during training. This is when triaining is terminated should decreasing validation accuracy be detected.\newline

\subsubsection{Link Prediction Results}
The link prediction accuracy results of the RNTN model, a nonlinear factorisation model compared against linear factorisation models, are presented in table 5.1:

\begin{figure}[H]
	\parbox{.5\linewidth}{
   		\caption{Wordnet Cost}
   		\centering
    		\includegraphics[width=0.45\textwidth, height=0.2\textheight]{Wordnet_Cost_Results_Early_Stopping}
		}
	\hfill
	\parbox{.5\linewidth}{
		\caption{Freebase Cost}
   		\centering
		\includegraphics[width=0.45\textwidth, height=0.2\textheight]{Freebase_Cost_Results}
		}
\end{figure}


\begin{figure}[H]
	\parbox{.5\linewidth}{
   		\caption{Wordnet Accuracy}
   		\centering
    		\includegraphics[width=0.45\textwidth, height=0.2\textheight]{Wordnet_Accuracy_Results_Early_Stopping}
		}
	\hfill
	\parbox{.5\linewidth}{
		\caption{Freebase Accuracy}
   		\centering
		\includegraphics[width=0.45\textwidth, height=0.2\textheight]{Freebase_Accuracy_Results}
		}
\end{figure}



\begin{table}[H]
	\caption{Link prediction accuracy on Wordnet and Freebase}
	\centering
	\begin{tabular}{lllllllllll}
  		\textbf{Model} & \textbf{Wordnet} & \textbf{Freebbase} & \textbf{Avg} \\
  		\hline
  		Distance Model & .683 & .610 & .647 \\
  		Hadamard Model & .800 & .688 & .744 \\
  		Single Layer Model & .760 & .853 & .807 \\
  		Bilinear Model & \textbf{.841} & \textbf{.877} & \textbf{.859} \\
  		Recursive Neural Tensor Network & .562 & .535 & .549 \\
  		\hline
  		Recursive Neural Tensor Network+ & .674 & .548 & .611 \\
	\end{tabular}
\end{table}

\section{HypER Convolutional Neural Networks}

\subsubsection{Model Summary} 
In order to overcome the expressiveness problems evident in the Freebase test dataset, more nonlinear relational factorisation approaches have been proposed \cite{ComplEx, Neural LP, TorusE}. \newline
HypER is a model that uses convolutional relational filters r that are convolved with the subject entity e1, producing an intermediate relational representation. The dot product of the relational representation is then taken with the object entity e2 to produce relation-specific score between the two entities. The HypER model only produces a latent relational representation for the subject entity e1, we extend this model to also produce a relational representation for the object entity e2, and call this model HpyER+. HypER+ thus produces entity-relational representations that approximate KG spacial locality. \newline
\subsubsection{Binary Cross Entropy Loss}
The binary cross entropy loss \cite{reference} is used to train the HypER Convolutional model. Like the RNTN model, the input consists of an entity-relational pair, where the entity is a subject entity and an object entity is presented as a target to complete the triple. A logit is generated for each sample and passed through through a logarithmic sigmoid or softmax function. Loss is generated by comparing the produced likelihood with the expected likelihood, 0 or 1. The sum of all losses is aggregated and back propagated through the network for parameter update. \newline

\subsubsection{Experimental Setup} 

We use the following benchmark knowledge graphs: Wordnet - a lexical database for English, it is a taxonomy with hypernyms (is-a) relationships, and synonym sets. \newline
Freebase - a large collaborative knowledge base consisting of data about the world composed mainly by its community members. It was an online collection of structured data harvested from many sources, including user-submitted wiki contributions. \newline
Visualisations of the respective knowledge graphs are presented below:

\begin{figure}[H]
  	\caption{WN18 Entities and Relations Graphplot}
   	\centering
    	\includegraphics[width=\textwidth]{WN18_Graph}
\end{figure}

\begin{figure}[H]
  	\caption{FB15k Entity and Relations Graphplot}
   	\centering
    	\includegraphics[width=\textwidth]{FB15k_Graph}
\end{figure}


We used the Pytorch framework to develop our model. This model is built on top of the HypER model introduced by (Balaˇzevi´c, Allen, and Hospedales 2018) ~\citep{balazevic2019hypernetwork}.  Randomly initialised entity and relational embeddings are used to initialise model training. These embeddings are dynamically adjusted during the training process to generate latent representations specific to the knowledge domain. Property counts for the respective knowledge graphs are presented below:

\begin{figure}[H]
	\parbox{.5\linewidth}{
   		\caption{WN18 Property Barplot}
   		\centering
    		\includegraphics[width=0.45\textwidth]{WN18_Counts}
		}
	\hfill
	\parbox{.5\linewidth}{
		\caption{Freebase Property Barplot}
   		\centering
    		\includegraphics[width=0.45\textwidth]{FB15k_Counts}
		}
\end{figure}


\begin{table}[H]
	\parbox{.5\linewidth}{
		\caption{WN18 Property Counts}
		\centering
		\begin{tabular}{lllllllllll}
  			\textbf{Property} & \textbf{Count}  \\
  			\hline
  			Entities & 40,943  \\
  			Relations & 18  \\
  			Triples & 151,442 \\
		\end{tabular}
		}
	\hfill
	\parbox{.5\linewidth}{
		\caption{Freebase Property Counts}
		\centering
		\begin{tabular}{lllllllllll}
  			\textbf{Property} & \textbf{Count}  \\
  			\hline
  			Entities & 14,951   \\
  			Relations & 1,345  \\
  			Triples & 592,213  \\
		\end{tabular}
		}
\end{table}


Summary statistics of the respective knowledge graphs Resource Description Framework (RDF) decomposition - subject, predicate, object - are presented below:

% Predicate

\begin{figure}[H]
	\parbox{.5\linewidth}{
   		\caption{WN18 Predicate Barplot}
   		\centering
    		\includegraphics[width=0.45\textwidth, height=0.2\textheight]{WN18_Predicate_Counts}
		}
	\hfill
	\parbox{.5\linewidth}{
		\caption{Freebase Predicate Barplot}
   		\centering
		\includegraphics[width=0.45\textwidth, height=0.2\textheight]{FB15k_Predicate_Counts}
		}
\end{figure}

\begin{table}[H]
	\parbox{.5\linewidth}{
		\caption{WN18 Predicate Statistics}
		\centering
		\begin{tabular}{lllllllllll}
  			\textbf{Statistic} & \textbf{Value}  \\
  			\hline
			Count & 18 \\
			Max & 37,221  \\
			Min & 86 \\
  			Median & 3,242.5  \\
  			IQR & 6,190.75  \\
		\end{tabular}
		}
	\hfill
	\parbox{.5\linewidth}{
		\caption{FB15k Predicate Statistics}
		\centering
		\begin{tabular}{lllllllllll}
  			\textbf{Statistic} & \textbf{Value}  \\
  			\hline
			Count & 1,345 \\
			Max & 19,764  \\
			Min & 1  \\
  			Median & 26  \\
  			IQR & 166  \\
		\end{tabular}
		}
\end{table}

% Subject

\begin{figure}[H]
	\parbox{.5\linewidth}{
   		\caption{WN18 Subject Barplot}
   		\centering
    		\includegraphics[width=0.45\textwidth, height=0.2\textheight]{WN18_Subject_Counts}
		}
	\hfill
	\parbox{.5\linewidth}{
		\caption{FB15k Subject Barplot}
   		\centering
		\includegraphics[width=0.45\textwidth, height=0.2\textheight]{FB15k_Subject_Counts}
		}
\end{figure}


\begin{table}[H]
	\parbox{.5\linewidth}{
		\caption{WN18 Subject Statistics}
		\centering
		\begin{tabular}{lllllllllll}
  			\textbf{Statistic} & \textbf{Value}  \\
  			\hline
			Count & 32,544 \\
			Max & 520 \\
			Min & 1 \\
  			Median & 3 \\
  			IQR & 2 \\
		\end{tabular}
		}
	\hfill
	\parbox{.5\linewidth}{
		\caption{FB15k Subject Statistics}
		\centering
		\begin{tabular}{lllllllllll}
  			\textbf{Statistic} & \textbf{Value}  \\
  			\hline
			Count &14,865 \\
			Max & 4,381 \\
			Min & 1 \\
  			Median & 27 \\
  			IQR & 32 \\
		\end{tabular}
		}
\end{table}

% Object

\begin{figure}[H]
	\parbox{.5\linewidth}{
   		\caption{WN18 Object Barplot}
   		\centering
    		\includegraphics[width=0.45\textwidth, height=0.2\textheight]{WN18_Object_Counts}
		}
	\hfill
	\parbox{.5\linewidth}{
		\caption{FB15k Object Barplot}
   		\centering
		\includegraphics[width=0.45\textwidth, height=0.2\textheight]{FB15k_Object_Counts}
		}
\end{figure}


\begin{table}[H]
	\parbox{.5\linewidth}{
		\caption{WN18 Object Statistics}
		\centering
		\begin{tabular}{lllllllllll}
  			\textbf{Statistic} & \textbf{Value}  \\
  			\hline
			Count & 32,543 \\
			Max & 520 \\
			Min & 1 \\
  			Median & 3 \\
  			IQR & 2 \\
		\end{tabular}
		}
	\hfill
	\parbox{.5\linewidth}{
		\caption{FB15k Object Statistics}
		\centering
		\begin{tabular}{lllllllllll}
  			\textbf{Statistic} & \textbf{Value}  \\
  			\hline
			Count & 14,930 \\
			Max & 9,645 \\
			Min & 1 \\
  			Median & 23 \\
  			IQR & 30 \\
		\end{tabular}
		}
\end{table}

For WN18, it can be seen that relations are skewed toward the relations "hyponym",  "hypernym", and "derivationally related from", with a maximum of 37,221 occurrences. \newline
FB15k relations are somewhat more uniform. We would expect reasonable performance across all relations for this knowledge graph. \newline
WN18 and FB15k subjects are somewhat uniform aside from a small number of high occurrences entities, with the median number of occurrences is 3 and 27 respectively, and with an IQR of 2 and 32 respectively. 
WN18 object occurrences are somewhat uniform. FB15k object occurrences are skewed, with the "United States" partaking in the highest number of facts. This is in comparison to a median object occurrence of 3 and an interquartile range of 23.

% Best Practise Deep Learning Techniques

\subsubsection{Hypothesis 1: \newline 
Recursive Neural Tensor Networks with Best Practise Deep Learning Techniques}
Model hyperparameter optimisation is implemented using random grid search. The the model was trained on a MacBook Pro 2015 with 8 cores, 16GB RAM, and 512GB SSD. \newline
We evaluate the model by ranking the accuracy scores of the predicted triples for the respective datasets. Code to reproduce: https://github.com/xhosaBoy/deep-knowledge-modelling

\subsubsection{Bias Variance Trade Off}
Machine learning models can suffer from overfitting \cite{reference} - when training has progressed too long, and the model parameters have been optimised for prediction on the training set. This leads to poor generalisation  and reveals itself in the form of increasing training accuracy, and decreasing validation accuracy. In order to overcome this problem, early stopping is often used during training. This is when triaining is terminated should decreasing validation accuracy be detected.\newline

\subsubsection{Link Prediction Results}
The link prediction accuracy results of the RNTN model, a nonlinear factorisation model compared against linear factorisation models, are presented in table 5.1:

\begin{figure}[H]
	\parbox{.5\linewidth}{
   		\caption{Wordnet Cost}
   		\centering
    		\includegraphics[width=0.45\textwidth, height=0.2\textheight]{Wordnet_Cost_Results_Early_Stopping}
		}
	\hfill
	\parbox{.5\linewidth}{
		\caption{Freebase Cost}
   		\centering
		\includegraphics[width=0.45\textwidth, height=0.2\textheight]{Freebase_Cost_Results}
		}
\end{figure}


\begin{figure}[H]
	\parbox{.5\linewidth}{
   		\caption{Wordnet Accuracy}
   		\centering
    		\includegraphics[width=0.45\textwidth, height=0.2\textheight]{Wordnet_Accuracy_Results_Early_Stopping}
		}
	\hfill
	\parbox{.5\linewidth}{
		\caption{Freebase Accuracy}
   		\centering
		\includegraphics[width=0.45\textwidth, height=0.2\textheight]{Freebase_Accuracy_Results}
		}
\end{figure}



\begin{table}[H]
	\caption{Link prediction accuracy on Wordnet and Freebase}
	\centering
	\begin{tabular}{lllllllllll}
  		\textbf{Model} & \textbf{Wordnet} & \textbf{Freebbase} & \textbf{Avg} \\
  		\hline
  		Distance Model & .683 & .610 & .647 \\
  		Hadamard Model & .800 & .688 & .744 \\
  		Single Layer Model & .760 & .853 & .807 \\
  		Bilinear Model & \textbf{.841} & \textbf{.877} & \textbf{.859} \\
  		Recursive Neural Tensor Network & .562 & .535 & .549 \\
  		\hline
  		Recursive Neural Tensor Network+ & .674 & .548 & .611 \\
	\end{tabular}
\end{table}

\section{HypER Convolutional Neural Networks with Pretrained Entity Embeddings}

\section{Leader Board Research Versus Hypothesis Research}

