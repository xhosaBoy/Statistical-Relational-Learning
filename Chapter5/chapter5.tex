%!TEX root = ../thesis.tex
%*******************************************************************************
%*********************************** Fifth Chapter *****************************
%*******************************************************************************

\chapter{Conclusions}  %Title of the First Chapter

\ifpdf
    \graphicspath{{Chapter5/Figs/Raster/}{Chapter5/Figs/PDF/}{Chapter5/Figs/}}
\else
    \graphicspath{{Chapter5/Figs/Vector/}{Chapter5/Figs/}}
\fi


%********************************** %First Section  **************************************

Statistical relational learning (SRL) is a relatively young subfield in artificial intelligence (AI). It is concerned with reasoning, and makes use of the structure found in relational data, often represented as graphs, along with statistical attribute representations of data, common in most machine learning (ML) approaches \unskip~\citep{koller2007introduction}. Motivation in this line research is driven by the ambition of realising artificial general intelligence (AGI), machine systems with human-like reasoning capability. A potentially useful application of such a technology is open-domain question answering (QA) \unskip~\citep{chen2017reading}, and another more speculative application is conversational AI \unskip~\citep{moon2019opendialkg, basu2019conversational}. Recent progress has been realised with a focus on multi-hop reasoning \unskip~\citep{asai2019learning, lin2018multi, qi2019answering}. This is a new SRL paradigm that attempts to directly imitate the step-by-step reasoning process of humans. A new dataset has also been proposed which aims to address some of the issues discussed in Chapter1 with standard link prediction benchmark metrics, HotpotQA \unskip~\citep{yang2018hotpotqa}. It is a more robust measure of assessing progress in knowledge graph question answering (KGQA), by enforcing explainable predictions. Latent feature modelling based link prediction may continue to play a role in KGQA, as a component, in a larger system of components of components, used for reasoning. \par   

\noindent Neural tensor factorisation shows great promise in further extending the performance of latent feature modelling based link prediction. The original insight to apply tensor decompositions to relational modelling, by Nickel et al. \unskip~\citep{nickel2011three}, was a watershed moment in SRL. It gave rise to a number of models \unskip~\citep{bordes2011learning, jenatton2012latent, socher2013reasoning, trouillon2016complex, hohenecker2017deep, dettmers2018convolutional, balazevic2019hypernetwork} which ultimately lead to to HypER+, seeing significant improvements in relation prediction performance in each subsequent model. There remains an open question as to the extent to which representation structure can continue to improve performance, for example HypER \unskip~\citep{balazevic2019hypernetwork} extends the concept of multi-dimensional inputs proposed by ConvE \unskip~\citep{dettmers2018convolutional}, by generating relational matrices as opposed to relying on relational vectors. A natural extension to this approach is generating entity matrices, and then potentially generating tensor representations for both. Such dense representations may better capture entity and relational concepts. The application of alternative entity-relation feature interaction operators is another natural extension. \unskip~\citep{trouillon2016complex, nickel2016holographic, dettmers2018convolutional} demonstrate that gains that can be realised using an alternative to the standard dot product when computing relation scores, relying on the Hermitian dot product, circular correlation and convolution respectively. An as yet unexplored avenue in latent feature modelling are stochastic operators \unskip~\citep{kingma2013auto}. Such operators directly encode uncertainty, and may produce relational score probabilities with more appropriate confidence measures. \par

\noindent Some progress in link prediction was recently achieved by (Nathani et al. 2019) using a graph modelling approach ~\citep{nathani2019learning}. They were able to achieve an accuracy performance of 46\% on FB15k-237. This represents an increase of 19\%, a staggering improvement. They were not able to replicate the results for WN18RR however, and achieved a performance of 36\%, 7\% below the latent feature modelling based result. (Pinter and Eisenstein 2018) ~\citep{pinter-eisenstein-2018-predicting} were able to achieve at SOTA performance on WN18RR of 45\%, an increase of 2\% on the latent feature model based approach. Graph modelling approaches clearly demonstrate potential in push link prediction performance forward and, unlike latent feature modelling, have yet to be substantially researched. They represent a potential alternative avenue of reasoning. \par

\noindent That being said, recent advances in latent feature modelling were achieved using new neural compositional model, specifically neural tensor factorisation. These models replaced the dot product between subject and predicate representations with a convolutional operator. It stands to reason that doing the same for compositional operator between the generated latent representation and object representation, could produce link prediction performance gains not yet achieved. This simple adjustment would produce an end-to-end neural compositional model which would extend entity-relational interaction expressiveness even further and increase model capacity. In pursuit of improved link prediction performance, in this dissertation we argued 1) machine learning methods are a sensible approach to reasoning about knowledge expressed in natural language. Latent feature modelling in SRL extends logical inference techniques, an early attempt at this problem, by allowing the relaxation of plausibility constraints - expressing facts using a measure of uncertainty, and retaining plausible facts, whilst discarding implausible facts. We argued that 2) Deep learning models can be used to improve latent feature modelling approaches to link prediction in SRL and 3) semantic information in pre-trained word vectors can be used used to model richer entity-relational interactions. \par

\noindent Deep learning methods have lead to the development of a number of successful latent feature modelling approaches for link prediction. Performance on traditional benchmark metrics now stands at 95\% and 79\% on WN18 and FB15k respectively. This has been achieved whilst striving to ensure algorithms remain scalable, with the intent of tractability. Performance on more modern datasets has not been as prolific, and state-of-the-art (SOTA) currently stands at 43\% and 25\% on WN18RR and FB15k-237 respectively. This suggests a lot of work remains in latent feature modellling, or alternative avenues of reasoning should be explored. \par

\noindent To this end, in \textbf{Chapter 2} we conducted a survey of deep learning models, specifically the convolutional and recurrent models. We examined their respective properties, as well as representational modelling strengths. Convolutional networks (CNN) are particularly adept at encoding complex interactions of dense representations very efficiently. This trait maps well to the dense entity and relation representations used in knowledge bases (KB), as well the complex interactions they take part in. Recurrent networks (RNN) on the other hand have strengths in modelling sequential data, and are useful for generating word representations. We concluded the chapter with a discussion on word vector generation strategies, exploring the Word2Vec models and algorithm. \par

\noindent In \textbf{Chapter 3} we reviewed an early successful attempt at link prediction using a neural compositional model: Recursive Neural Tensor Networks (NTN). The NTN was successful given the insight that compositionally of language gives rise to semantics. This insight inspired recursive networks (RCN) which are applied in the NTN model to enhance the entity-relational interaction representation. We reason simple adjustments to the original NTN training algorithm, taking advantage of modern deep learning techniques, should improve link prediction performance. \newpage 

\noindent We also analyse the HypER model, a convolutional model with that uses a hypernetwork for parameter generation. This model was inspired by another convolutional model ConvE, and both use a convolutional operator to increase entity-relational interaction expressiveness of previous latent feature models. We reason hypernetworks introduce covariate shift, and pre-trained word vectors could enhance entity and relation representational modelling. \par

\noindent In \textbf{Chapter 4} we implement an updated model to NTN in Tensorflow. We apply adaptive moment estimation (Adam) and hyperparameter random search. This results in a performance improvement in link prediction accuracy, validating our reasoning that simply adjusting training algorithms with modern deep learning techniques can result in model performance improvement. We then adjust the HypER model to compensate for covariate shift introduced by the hypernetwork, and introduce HypER+. Again we see performance improvements in link prediction performance, validating our reasoning of potential distribution drift experienced by the neural compositional model during training. Finally we extend HypER+ by initialising entity and relation representations using pre-trained GloVe word vectors. We see slight improvement our link prediction performance, validating our final reasoning of the entity-relational interaction modelling gain that can be achieved for semantically rich representations. \par

\noindent Despite this progress, a lot of work still remains in order to improve link prediction accuracy above as little as 50\% on modern benchmark datasets. Latent feature modelling may not yet been reached its limits performance, given the opportunity of applying ever more simultaneously both expressive and efficient interaction modelling operators. Given the progress in computing power, one could further lossen the requirement of efficiency provided solutions remain analytically tractable (analytical tractability also may not be necessary depending on application requirement).\par

\noindent A somewhat more attractive avenue is graph modelling.  ~\citep{nathani2019learning} and ~\citep{pinter-eisenstein-2018-predicting} demonstrate the potential this line of research has in substantially improving link prediction results. And of course a hybridisation of methods may improve results even further. Recommended future work would be the combination of graph convolutional networks (GCN) with attention using gated recurrent units (GRU). Such a combination promises to take advantage of the strengths of both CNNs and RNNs, whilst taking the geometric structure of the graph into account. We leave the exploration of this hypothesis to future researchers.
