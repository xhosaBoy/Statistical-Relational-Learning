%!TEX root = ../thesis.tex
%*******************************************************************************
%*********************************** Fifth Chapter *****************************
%*******************************************************************************

\chapter{Conclusions}  %Title of the First Chapter

\ifpdf
    \graphicspath{{Chapter5/Figs/Raster/}{Chapter5/Figs/PDF/}{Chapter5/Figs/}}
\else
    \graphicspath{{Chapter5/Figs/Vector/}{Chapter5/Figs/}}
\fi


%********************************** %First Section  **************************************

Statistical relational learning (SRL) is a relatively young subfield in artificial intelligence (AI).\ It is concerned with reasoning, and makes use of the structure found in relational data, often represented as graphs, along with statistical attribute representations of data, common in many machine learning (ML) approaches \unskip~\citep{koller2007introduction}.\ Motivation in this line research is driven by the ambition of realising artificial general intelligence; machine systems with human-like reasoning capability. A potentially useful application is open-domain question answering (QA) \unskip~\citep{chen2017reading}, and another more speculative application is conversational AI \unskip~\citep{moon2019opendialkg, basu2019conversational}.\ Recent progress has been realised with a focus on multi-hop reasoning \unskip~\citep{asai2019learning, lin2018multi, qi2019answering}.\ This is a new SRL paradigm that attempts to directly imitate the step-by-step reasoning process of humans.\ A new dataset called HotpotQA \unskip~\citep{yang2018hotpotqa} has also been proposed which aims to address some of the issues discussed in Chapter 1 with standard link prediction benchmark metrics.\ It is a more robust measure of assessing progress in knowledge graph question answering (KGQA), by enforcing explainable predictions.\ Latent feature modelling based link prediction may continue to play a role in KGQA, as a component, in a larger reasoning system. \par   

\noindent Neural tensor factorisation shows great promise in further extending the performance of latent feature modelling based link prediction. The original insight to apply tensor decompositions to relational modelling, by Nickel et al. \unskip~\citep{nickel2011three}, was a watershed moment in SRL. It gave rise to a number of models \unskip~\citep{bordes2011learning, jenatton2012latent, socher2013reasoning, trouillon2016complex, hohenecker2017deep, dettmers2018convolutional, balazevic2019hypernetwork} which ultimately lead to HypER+, seeing significant improvements in relation prediction performance in each subsequent model. There remains an open question as to the extent to which representation structure can continue to improve performance, for example HypER \unskip~\citep{balazevic2019hypernetwork} extends the concept of multi-dimensional inputs proposed by ConvE \unskip~\citep{dettmers2018convolutional}, by generating relational matrices as opposed to relying on relational vectors. A natural extension to this approach is generating entity matrices, and then potentially generating tensor representations for both. Such dense representations may better capture entity and relational concepts. The application of alternative entity-relation feature interaction operators is another natural extension. Others have demonstrated \unskip~\citep{trouillon2016complex, nickel2016holographic, dettmers2018convolutional} that gains that can be realised using an alternative to the standard dot product when computing relation scores, relying on the Hermitian dot product, circular correlation and convolution respectively. An as yet unexplored avenue in latent feature modelling is the use of stochastic operators \unskip~\citep{kingma2013auto}. Such operators directly encode uncertainty, and may produce relational score probabilities with more appropriate confidence measures. \par

\noindent Deep learning methods have lead to the development of a number of successful approaches to neural tensor factorisation. Performance is now at $ 43.5\% $ and $ 25.5\% $ on the benchmark WN18RR and FB15k-237 KGs, respectively. In pursuit of improved link prediction performance, in this thesis we argued that (1) ML methods are a sensible approach to reasoning about knowledge expressed in natural language. Specifically, latent feature modelling in SRL enables the generation of a measure of confidence for relation prediction, expressing possible links in a KG using probabilities, and ranking those measures to derive the set of facts with the highest potential for being true. We also argued that (2) deep learning models can be used to improve latent feature modelling approaches to link prediction in SRL, and that (3) semantic information in pre-trained word vectors can be used to model richer entity-relation feature interactions. \par

\noindent To this end, in \textbf{Chapter 2} we conducted a survey of deep learning models, specifically the convolutional and recurrent models.\ We examined their respective properties, as well as representational modelling strengths. Convolutional networks (CNs) are particularly adept at efficiently encoding complex interactions of dense representations. This trait maps well to the dense entity and relation representations used in KGs, as well as the complex interactions they take part in. Recurrent networks (RNs) on the other hand have strengths in modelling sequential data, and are useful for generating word vectors. We then gave quick overviews of recursive networks (RCNs) and hypernetworks, along with their respective modelling strengths. We concluded the chapter with a discussion on ML model regularisation strategies, as well as a discussion on building language models. \par

\noindent In \textbf{Chapter 3} we reviewed an early successful attempt at link prediction using a neural compositional model: the recursive neural tensor network (NTN). The NTN was successful given the insight that compositionally of language gives rise to semantics.\ This insight inspired RCNs which are applied in the NTN model to enhance the entity-relational interaction representation. We reasoned that simple adjustments to the original NTN training algorithm, making use of ML techniques known to improve performance, should result in higher prediction accuracy. \par 

\noindent We also analysed the HypER model, a convolutional model that uses a hypernetwork for relation parameter generation. This model was inspired by another convolutional model called ConvE, and both use a convolutional operator to increase the entity-relational interaction expressiveness of previous tensor factorisation models.\ We reasoned that hypernetworks may introduce covariate shift, and relation normalisation using batch normalisation may compensate for this.\ We also reasoned that pre-trained word vectors may enhance entity and relation representation modelling. \par

\noindent In \textbf{Chapter 4} we implemented an updated version of the NTN in TensorFlow.\ We applied early stopping, adaptive moment estimation (Adam) optimisation, and hyperparameter random search. This resulted in a performance improvement in link prediction accuracy, validating our reasoning that simply adjusting training algorithms with ML techniques known to improve performance, may result in increased relation prediction accuracy.\ We then adjusted the HypER model to compensate for covariate shift introduced by a hypernetwork, and thus introduced HypER+. Again we saw improvements in link prediction performance, validating our reasoning of distribution drift in relation latent representations experienced during training, resulting in suboptimal relation parameters. Finally we extended HypER+ by initialising entity and relation embeddings using pre-trained word vectors from the GloVe language model. We saw significant relation prediction improvement on the WN18RR KG, as well as relation prediction improvement on the FB15k-237 KG. Our model is also the state-of-the-art (SOTA) latent feature modelling approach to link prediction at the time of writing, validating our final reasoning of the utility of semantically rich word vectors in relation prediction. \par

\noindent Some progress in link prediction was recently achieved using the graph modelling \unskip ~\citep{Nickel_2016} paradigm.\ A SOTA Hit@1 accuracy of $ 46\% $ was achieved on FB15k-237 by Nathani et al. \unskip ~\citep{nathani2019learning}, compared to an accuracy of $ 25.5\% $ achieved by HypER+ with pre-trained word vectors. This represents an increase of $ 20.5 \% $ \unskip ~\citep{ruderNLPProg}, which is a staggering improvement. That level of performance was however not realised on WN18RR, with the model achieving a Hit@1 accuracy of $ 36.1\% $, compared to $ 43.5 \% $ achieved by HypER+ with pre-trained word vectors; a difference in performance of $ 7.4\% $. Pinter and Eisenstein \unskip ~\citep{pinter-eisenstein-2018-predicting}, using another graph modelling approach, were able to achieve a SOTA performance on WN18RR of $ 45.37\% $; an increase in performance of $ 1.87\% $ on HypER+ with pre-trained word vectors. Graph modelling approaches clearly demonstrate potential in pushing link prediction performance forward. The strength in graph modelling approaches could be exploiting a similar idea to the one used to construct the GloVe \unskip ~\citep{pennington2014glove} language model, incorporating both local and global context. Inductive probabilistic logic programming (IPLP) also shows promise \unskip ~\citep{dong2019neural, manhaeve2018deepproblog}, although research in this paradigm is a lot more sparse.\ It would seem graph modelling, as opposed to latent feature modelling, may provide the biggest contribution to SRL in the near-term. 
