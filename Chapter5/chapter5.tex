%!TEX root = ../thesis.tex
%*******************************************************************************
%*********************************** Fifth Chapter *****************************
%*******************************************************************************

\chapter{Conclusions}  %Title of the First Chapter

\ifpdf
    \graphicspath{{Chapter5/Figs/Raster/}{Chapter5/Figs/PDF/}{Chapter5/Figs/}}
\else
    \graphicspath{{Chapter5/Figs/Vector/}{Chapter5/Figs/}}
\fi


%********************************** %First Section  **************************************

Statistical relational learning (SRL) is a relatively young subfield in artificial intelligence (AI). It is concerned with reasoning, and makes use of the structure found in relational data, often represented as graphs, along with statistical attribute representations of data, common in most machine learning (ML) approaches \unskip~\citep{koller2007introduction}. Motivation in this line research is driven by the ambition of realising artificial general intelligence (AGI), machine systems with human-like reasoning capability. A potentially useful application of such a technology is open-domain question answering (QA) \unskip~\citep{chen2017reading}, and another more speculative application is conversational AI \unskip~\citep{moon2019opendialkg, basu2019conversational}. Recent progress has been realised with a focus on multi-hop reasoning \unskip~\citep{asai2019learning, lin2018multi, qi2019answering}. This is a new SRL paradigm that attempts to directly imitate the step-by-step reasoning process of humans. A new dataset has also been proposed which aims to address some of the issues discussed in Chapter1 with standard link prediction benchmark metrics, HotpotQA \unskip~\citep{yang2018hotpotqa}. It is a more robust measure of assessing progress in knowledge graph question answering (KGQA), by enforcing explainable predictions. Latent feature modelling based link prediction may continue to play a role in KGQA, as a component, in a larger system of components of components, used for reasoning. \par   

\noindent Neural tensor factorisation shows great promise in further extending the performance of latent feature modelling based link prediction. The original insight to apply tensor decompositions to relational modelling, by Nickel et al. \unskip~\citep{nickel2011three}, was a watershed moment in SRL. It gave rise to a number of models \unskip~\citep{bordes2011learning, jenatton2012latent, socher2013reasoning, trouillon2016complex, hohenecker2017deep, dettmers2018convolutional, balazevic2019hypernetwork} which ultimately lead to to HypER+, seeing significant improvements in relation prediction performance in each subsequent model. There remains an open question as to the extent to which representation structure can continue to improve performance, for example HypER \unskip~\citep{balazevic2019hypernetwork} extends the concept of multi-dimensional inputs proposed by ConvE \unskip~\citep{dettmers2018convolutional}, by generating relational matrices as opposed to relying on relational vectors. A natural extension to this approach is generating entity matrices, and then potentially generating tensor representations for both. Such dense representations may better capture entity and relational concepts. The application of alternative entity-relation feature interaction operators is another natural extension. \unskip~\citep{trouillon2016complex, nickel2016holographic, dettmers2018convolutional} demonstrate that gains that can be realised using an alternative to the standard dot product when computing relation scores, relying on the Hermitian dot product, circular correlation and convolution respectively. An as yet unexplored avenue in latent feature modelling are stochastic operators \unskip~\citep{kingma2013auto}. Such operators directly encode uncertainty, and may produce relational score probabilities with more appropriate confidence measures. \par

\noindent Some progress in link prediction was recently achieved using the graph modelling \unskip ~\citep{Nickel_2016} paradigm. A state-of-the-art (SOTA) Hit@1 accuracy of $ 46\% $ was achieved on FB15k-237 by \unskip ~\citep{nathani2019learning}, compared to an accuracy of $ 25.5\% $ achieved by HypER+ with pre-trained word vectors. This represents an increase of $ 20.5 \% $ \unskip ~\citep{ruderNLPProg}, which is a staggering improvement. That level of performance was however not realised on WN18RR, with the model achieving a Hit@1 accuracy of $ 36.1\% $, compared with $ 43.5 \% $ achieved by HypER+ with pre-trained word vectors, a difference in performance of $ 7.4\% $. \unskip ~\citep{pinter-eisenstein-2018-predicting}, using another graph modelling approach, were able to achieve a SOTA performance on WN18RR of $ 45.37\% $, an increase in performance of $ 1.87\% $ on HypER+ with pre-trained word vectors. Graph modelling approaches clearly demonstrate potential in pushing link prediction performance forward. The strength in graph modelling approaches could be exploiting a similar idea to the one used to construct the GloVe \unskip ~\citep{pennington2014glove} language model, incorporating both local and global context. Inductive probabilistic logic programming (IPLP) also shows promise \unskip ~\citep{dong2019neural, manhaeve2018deepproblog}, although research in this paradigm is a lot more sparse.\ It would seem graph modelling, as opposed to latent feature modelling, may provide the biggest contribution to SRL in the near-term. \par

\noindent Deep learning methods have lead to the development of a number of successful latent feature modelling approaches. Performance on standard datasets is now at $ 43.5\% $ and $ 25.5\% $ on the WN18RR and FB15k-237 KGs respectively. In pursuit of improved link prediction performance, in this thesis we argued that (1) ML methods are a sensible approach to reasoning about knowledge expressed in natural language. Specifically that latent feature modelling in SRL enables the generation of a measure of confidence for relation prediction, expressing possible links in a KG using probabilities, and ranking those measures to derive the set of facts with the highest potential for being true. We also argued that (2) deep learning models can be used to improve latent feature modelling approaches to link prediction in SRL, and that (3) semantic information in pre-trained word vectors can be used to model richer entity-relation feature interactions. \par

\noindent To this end, in \textbf{Chapter 2} we conducted a survey of deep learning models, specifically the convolutional and recurrent models. We examined their respective properties, as well as representational modelling strengths. Convolutional networks (CNN) are particularly adept at encoding complex interactions of dense representations very efficiently. This trait maps well to the dense entity and relation representations used in knowledge bases (KB), as well the complex interactions they take part in. Recurrent networks (RNN) on the other hand have strengths in modelling sequential data, and are useful for generating word representations. We concluded the chapter with a discussion on word vector generation strategies, exploring the Word2Vec models and algorithm. \par

\noindent In \textbf{Chapter 3} we reviewed an early successful attempt at link prediction using a neural compositional model: Recursive Neural Tensor Networks (NTN). The NTN was successful given the insight that compositionally of language gives rise to semantics. This insight inspired recursive networks (RCN) which are applied in the NTN model to enhance the entity-relational interaction representation. We reason simple adjustments to the original NTN training algorithm, taking advantage of modern deep learning techniques, should improve link prediction performance. \par 

\noindent We also analyse the HypER model, a convolutional model with that uses a hypernetwork for parameter generation. This model was inspired by another convolutional model ConvE, and both use a convolutional operator to increase entity-relational interaction expressiveness of previous latent feature models. We reason hypernetworks introduce covariate shift, and pre-trained word vectors could enhance entity and relation representational modelling. \par

\noindent In \textbf{Chapter 4} we implement an updated model to NTN in Tensorflow. We apply adaptive moment estimation (Adam) and hyperparameter random search. This results in a performance improvement in link prediction accuracy, validating our reasoning that simply adjusting training algorithms with modern deep learning techniques can result in model performance improvement. We then adjust the HypER model to compensate for covariate shift introduced by the hypernetwork, and introduce HypER+. Again we see performance improvements in link prediction performance, validating our reasoning of potential distribution drift experienced by the neural compositional model during training. Finally we extend HypER+ by initialising entity and relation representations using pre-trained GloVe word vectors. We see slight improvement our link prediction performance, validating our final reasoning of the entity-relational interaction modelling gain that can be achieved for semantically rich representations. \par

\noindent Despite this progress, a lot of work still remains in order to improve link prediction accuracy above as little as 50\% on modern benchmark datasets. Latent feature modelling may not yet been reached its limits performance, given the opportunity of applying ever more simultaneously both expressive and efficient interaction modelling operators. Given the progress in computing power, one could further lossen the requirement of efficiency provided solutions remain analytically tractable (analytical tractability also may not be necessary depending on application requirement).\par

\noindent A somewhat more attractive avenue is graph modelling.  ~\citep{nathani2019learning} and ~\citep{pinter-eisenstein-2018-predicting} demonstrate the potential this line of research has in substantially improving link prediction results. And of course a hybridisation of methods may improve results even further. Recommended future work would be the combination of graph convolutional networks (GCN) with attention using gated recurrent units (GRU). Such a combination promises to take advantage of the strengths of both CNNs and RNNs, whilst taking the geometric structure of the graph into account. We leave the exploration of this hypothesis to future researchers.
