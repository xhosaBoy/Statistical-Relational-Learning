%!TEX root = ../thesis.tex
%*******************************************************************************
%*********************************** Fifth Chapter *****************************
%*******************************************************************************

\chapter{Conclusions}  %Title of the First Chapter

\ifpdf
    \graphicspath{{Chapter5/Figs/Raster/}{Chapter5/Figs/PDF/}{Chapter5/Figs/}}
\else
    \graphicspath{{Chapter5/Figs/Vector/}{Chapter5/Figs/}}
\fi


%********************************** %First Section  **************************************

Statistical relational learning (SRL) is a relatively young field artificial intelligence (AI). The field is concerned with reasoning about knowledge expressed as a graph, and tries to solve the core task of open domain question answering. A number of exciting applications can be realised should this problem be addressed, including an evolution of search engines, scientific recommender systems and conversational AI. Link prediction is the preferred SRL technique used for this core task, and most recently has been delivered using latent feature modelling. \par

\noindent Deep learning methods have lead to the development of a number of successful latent feature modelling approaches for link prediction. Performance on traditional benchmark metrics now stands at 95\% and 79\% on WN18 and FB15k respectively. This has been achieved whilst striving to ensure algorithms remain scalable, with the intent of tractability. Performance on more modern datasets has not been as prolific, and state-of-the-art (SOTA) currently stands at 43\% and 25\% on WN18RR and FB15k-237 respectively. This suggests a lot of work remains in latent feature modellling, or alternative avenues of reasoning should be explored. \par

\noindent Some progress in link prediction was recently achieved by (Nathani et al. 2019) using a graph modelling approach ~\citep{nathani2019learning}. They were able to achieve an accuracy performance of 46\% on FB15k-237. This represents an increase of 19\%, a staggering improvement. They were not able to replicate the results for WN18RR however, and achieved a performance of 36\%, 7\% below the latent feature modelling based result. (Pinter and Eisenstein 2018) ~\citep{pinter-eisenstein-2018-predicting} were able to achieve at SOTA performance on WN18RR of 45\%, an increase of 2\% on the latent feature model based approach. Graph modelling approaches clearly demonstrate potential in push link prediction performance forward and, unlike latent feature modelling, have yet to be substantially researched. They represent a potential alternative avenue of reasoning. \newpage

\noindent That being said, recent advances in latent feature modelling were achieved using new neural compositional model, specifically neural tensor factorisation. These models replaced the dot product between subject and predicate representations with a convolutional operator. It stands to reason that doing the same for compositional operator between the generated latent representation and object representation, could produce link prediction performance gains not yet achieved. This simple adjustment would produce an end-to-end neural compositional model which would extend entity-relational interaction expressiveness even further and increase model capacity. In pursuit of improved link prediction performance, in this dissertation we argued 1) machine learning methods are a sensible approach to reasoning about knowledge expressed in natural language. Latent feature modelling in SRL extends logical inference techniques, an early attempt at this problem, by allowing the relaxation of plausibility constraints - expressing facts using a measure of uncertainty, and retaining plausible facts, whilst discarding implausible facts. We argued that 2) Deep learning models can be used to improve latent feature modelling approaches to link prediction in SRL and 3) semantic information in pre-trained word vectors can be used used to model richer entity-relational interactions. \par

\noindent To this end, in \textbf{Chapter 2} we conducted a survey of deep learning models, specifically the convolutional and recurrent models. We examined their respective properties, as well as representational modelling strengths. Convolutional networks (CNN) are particularly adept at encoding complex interactions of dense representations very efficiently. This trait maps well to the dense entity and relation representations used in knowledge bases (KB), as well the complex interactions they take part in. Recurrent networks (RNN) on the other hand have strengths in modelling sequential data, and are useful for generating word representations. We concluded the chapter with a discussion on word vector generation strategies, exploring the Word2Vec models and algorithm. \par

\noindent In \textbf{Chapter 3} we reviewed an early successful attempt at link prediction using a neural compositional model: Recursive Neural Tensor Networks (NTN). The NTN was successful given the insight that compositionally of language gives rise to semantics. This insight inspired recursive networks (RCN) which are applied in the NTN model to enhance the entity-relational interaction representation. We reason simple adjustments to the original NTN training algorithm, taking advantage of modern deep learning techniques, should improve link prediction performance. \newpage 

\noindent We also analyse the HypER model, a convolutional model with that uses a hypernetwork for parameter generation. This model was inspired by another convolutional model ConvE, and both use a convolutional operator to increase entity-relational interaction expressiveness of previous latent feature models. We reason hypernetworks introduce covariate shift, and pre-trained word vectors could enhance entity and relation representational modelling. \par

\noindent In \textbf{Chapter 4} we implement an updated model to NTN in Tensorflow. We apply adaptive moment estimation (Adam) and hyperparameter random search. This results in a performance improvement in link prediction accuracy, validating our reasoning that simply adjusting training algorithms with modern deep learning techniques can result in model performance improvement. We then adjust the HypER model to compensate for covariate shift introduced by the hypernetwork, and introduce HypER+. Again we see performance improvements in link prediction performance, validating our reasoning of potential distribution drift experienced by the neural compositional model during training. Finally we extend HypER+ by initialising entity and relation representations using pre-trained GloVe word vectors. We see slight improvement our link prediction performance, validating our final reasoning of the entity-relational interaction modelling gain that can be achieved for semantically rich representations. \par

\noindent Despite this progress, a lot of work still remains in order to improve link prediction accuracy above as little as 50\% on modern benchmark datasets. Latent feature modelling may not yet been reached its limits performance, given the opportunity of applying ever more simultaneously both expressive and efficient interaction modelling operators. Given the progress in computing power, one could further lossen the requirement of efficiency provided solutions remain analytically tractable (analytical tractability also may not be necessary depending on application requirement).\par

\noindent A somewhat more attractive avenue is graph modelling.  ~\citep{nathani2019learning} and ~\citep{pinter-eisenstein-2018-predicting} demonstrate the potential this line of research has in substantially improving link prediction results. And of course a hybridisation of methods may improve results even further. Recommended future work would be the combination of graph convolutional networks (GCN) with attention using gated recurrent units (GRU). Such a combination promises to take advantage of the strengths of both CNNs and RNNs, whilst taking the geometric structure of the graph into account. We leave the exploration of this hypothesis to future researchers.
