%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Third Chapter **********************************
%*******************************************************************************
\chapter{Neural Tensor Factorisation}


% **************************** Define Graphics Path **************************

\ifpdf
    \graphicspath{{Chapter3/Figs/Raster/}{Chapter3/Figs/PDF/}{Chapter3/Figs/}}
\else
    \graphicspath{{Chapter3/Figs/Vector/}{Chapter3/Figs/}}
\fi

\section{Link Prediction}
Link prediction aims to rank plausible entity relationships. In latent feature modelling, it is posed as a classification task. These ranks are computed by scoring entity-relational pairs against all entities within the knowledge base. Relationships that rank highly become potential new facts that have been discovered and generate new knowledge within the knowledge base. Similar to inductive logic \citet{Rea85,Ancey1996}, these facts need to be coherent, consistent and are not contradictory. Latent feature models contain statistical properties such as the capability to model a likelihood distribution over candidate entity relationships. This is.a more flexible modelling approach that provides a measure "confidence" in a fact, given the magnitude of the computed relationship score. 

\subsection{Entity-Relational Classification}
% e-r-e
Entity-relational classification is a subsection of the more general ranking problem in machine learning (ML) \citet{ranking, ranking ranking}. Standard classification involves the determining of the most appropriate categories in which entity belongs. If a logistic approach is used, a logit distribution across candidate classes is generated. A logit is referred to as an inverse probability \citet{wikipedia}, a magnitude with is the passed thought a thresholding logarithmic sigmoid function to generate a likelihood, or a probability in a finite number of mutually exclusive classes.. The category with the highest logit associated with is then determined to be the most likely category in which the entity belongs. In the ranking. 

\section{Nonlinear Factorisation Models}
% Tensor factorisation
Tensor facorisation makes use of explicit relational representations, in contrast to implicit relational representations such as SVD. The relational representations are expressed as matrices, which are used to generate subject entity relational (ER) transformations by computing the inner product between them respectively. An inner product of the ER representation and object entity is then taken to produce a relational plausibility magnitude. The tensor factorisation algorithm thus makes use of linear representations as well as linear operations to compute relational scores. 
% MLP relational factorisation
Deep learning modes have the capability to build useful latent feature representations, and given previous approaches to build relational representations using matrices, research was conducted to explore replacing relational matrices, with relational neural networks. E-MLP and ER-MLP were early attempts at relational factorisation using neural networks. Computing relational scores using these approaches is computationally expensive and prone to overfitting \cite{reference}. Recursive neural tensor networks were proposed as an alternative, however suffer from limited expressive power \cite{reference}.
% CNN relational factorisation
Convolutional neural networks were then used to model relational representations. In this approach, CNN filters explicitly model relations, and subject and object entities are modeled using real-valued feature vectors.. A convolutional operation is then performed using the the relational filters and subject entity. The generated ER representation is then flattened and an inner product is taken with the object entity to compute a relational score. ConvE and HypER are CNN approaches to relational factorisation. 

\subsection{Models}
what they are, and how we got here, trial and error
\begin{table}[H]
\centering
\begin{tabular}{lllllllllll}
  \textbf{Model} & \textbf{Scoring Function} \\
  \hline
  RESCAL (Nickel, Tresp, and Kriegel 2011) & $e^T_1W_r e_2$  \\
  TransE (Bordes et al. 2013) & $|| e_1 + w_r - e_2 ||$ \\
  NTN (Socher et al. 2013) & $u^T_r f(e_1W_r^{[1..k]} e_2 + V_r \begin{bmatrix}e_1 \\ e_2\end{bmatrix} + b_r)$ \\
  HolE (Nickel, Rosasco, and Poggio 2016) & $r^T_p(e_s * e_o)$ \\
  HypER (Balazevicl, Allen, and Hospedales 2018) & $f(vec(e_1 * vec^{-1}(w_rH))W)e_2$ \\
  HypER+ (Magangane and Brink 2019) & $f(vec(e_1 * vec^{-1}(w_rH))W)f(vec(e_2 * vec^{-1}(w_rH))W)$
\end{tabular}
 \caption {Scoring functions of link prediction models. $*$ is the convolutional operator $F_r = vec^{-1}(w_rH)$ the matrix of relation specific convolutional filters, $f$ is a non-linear function}
\end{table}

\section{Model Analysis}
Latent feature models determine factual plausibility by ranking all possible entity relationships within a knowledge graph. This is done by scoring all entities against against an ER representation, and then ranking those scores in descending order. To evaluate the performance of the latent feature model, a number of benchmark ranking metrics are used \cite{reference}. These metrics include the Hit@10 accuracy, Hit@3 accuracy, Hit@1 accuracy, Mean Rank, and Mean Reciprocal Rank. Definitions for each of the metrics are as follows:
\begin{itemize}
	\item \textbf{Hit@10 accuracy}: Occurs when the target subject entity appears within the top ten ranked subject entities when scored against an ER representation
	\item \textbf{Hit@3 accuracy}: Occurs when the target subject entity appears within the top three ranked subject entities when scored against an ER representation
	\item \textbf{Hit@1 accuracy}: Occurs when the target subject entity appears as the top top ranked subject entity when scored against an ER representation
	\item \textbf{Mean Rank}: For all questions asked within the test dataset, what is the average target subject entity rank. Where a smaller value is better and a desired value of close to 1, which is the minimum.
	\item \textbf{Mean Reciprocal Rank}: The inverse of the mean rank. Where a larger value is better with a desired metric of close to 1, which is the maximum.
\end{itemize}

\section{Nonlinear Factorisation Hypothesis}
and here I write more \dots
Progress in ER classification using latent feature models has tracked progress made in deep learning model development. The current state-of-the art in image classification architecture makes use of fundamental convolutional architectures \cite{reference}. The current state-of-the in natural language modelling makes use of contextual word embeddings \cite{reference}. A potential approach to further improve the performance of latent feature models is to take context into account when selecting entities. A practical implementation of this approach is using distinct subject and object representations for scoring facts. This has the effect of taking into consideration the directionality of the the entity similar to the approach considered in \cite{Deep Knowledge Models}. A new model using this approach can be developed using the current state-of-the-art latent feature model, HypER, as a basis, and simply incorporate the use use of distinct subject and object entity representations. This new model, Hyper+, can then be evaluated using the model analysis metrics discussed above. 

\subsection{Training Algorithm}

