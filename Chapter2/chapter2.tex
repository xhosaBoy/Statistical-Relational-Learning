%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Second Chapter *********************************
%*******************************************************************************

\chapter{Deep learning}

\ifpdf
     \graphicspath{{Figs/Chapter2/}}
\else
    \graphicspath{{Chapter2/Figs/Vector/}{Chapter2/Figs/}}
\fi

There are many tasks which are hard to write algorithms for. For example, writing an algorithm that identifies a type of fruit in a picture of the fruit is a challenging undertaking. One could try to do this by asking a number of yes or no questions to help whittle down the possibilities. Sensible questions might be: "is the fruit round?", "is the fruit green?", and "is the fruit's surface rough?". The problem with this approach is that it is very brittle. More questions could be introduced to disambiguate more cases, but the large variations in the appearance of fruit can cause the algorithm to become confused. Writing rules in this way is also not scalable. There are many types of fruits, and writing questions to determine each one quickly becomes intractable. \par
 
\noindent Machine learning (ML) can be used to solve such tasks, as well as others like weather prediction, stock price forecasting and risk modelling \unskip ~\citep{hastie2009elements}. ML is the process of using data to build prediction models, where these models typically output discrete (classification) or continuous (regression) values. There are various learning paradigms \unskip ~\citep{murphy2012machine}, including supervised learning, unsupervised learning and reinforcement learning, to train models from data in order to perform a specific task. The models can be divided into three classes, namely shallow, deep and probabilistic ~\citep{hastie2009elements, murphy2012machine}. \par

\noindent This chapter presents a discussion on the supervised learning paradigm, a description of deep convolutional \unskip ~\citep{lecun1998gradient} and recurrent network \unskip ~\citep{werbos1988generalization} neural network models, and a discussion on regularisation techniques for deep models, including dropout \unskip ~\citep{srivastava2014dropout} and batch normalisation \unskip ~\citep{ioffe2015batch}. 


%********************************** %First Section  **************************************

\section{Supervised learning}

An ML prediction model takes as input a set of features. These features are attributes of a data sample. For example in the fruit prediction task mentioned above, input features might be characteristics such as shape, colour, texture and size. These inputs are provided to a model that maps them to outputs. In the example above, these outputs would be types of fruit, and could represent values such corresponding to orange, apple, strawberry, pineapple, etc. The model is then trained to identify the correct fruit (output) based on the features (input) it receives, from a collection of input-output examples, a type of training called supervised learning \citep{bishop2006pattern, hastie2009elements, murphy2012machine}.

\begin{figure}[H]
   	\centering
    	\includegraphics[width=\textwidth]{supervised_learning}
	\caption{A supervised learning model is trained to map input features to expected output.}
\end{figure}

\noindent Features can be continuous valued or discrete, where discrete values are referred to as categorical variables. Outputs can also be continuous or discrete, where discrete outputs are known as classes. Example input-output pairs comprise a training dataset which is represented as, \begin{math} D = \{(x_i, y_i)\}_{i=1}^N \end{math}, where \begin{math} D \end{math} is an dataset, \begin{math} x_i \end{math} is the input feature and \begin{math} y_i \end{math} is its expected output label. The model has to learn a general mapping from input features to output labels. \par

\noindent In the fruit example, the model will attempt to build decision boundaries between fruit classes in the feature space. We can visualise this process in a 2-dimensional setting using two fruit features, say shape and colour. Figure 2.2 shows an example of synthetically generated samples in this 2D feature space and a potential decision boundary.

\begin{figure}[H]
   	\centering
    	\includegraphics[width=0.8\textwidth]{oranges_and_apples_decision_boundary}
	\caption{A model decision boundary between two classes.}
\end{figure}

\noindent Data is assumed to be generated by the same underlying process, and is divided into two classes: independent and identically distributed (IID), and non-IID. In the context of IID data, samples are independent and therefore order invariant; whilst in the context of non-IID data, samples are dependent and order matters. The class of data can determine the best choice of model for prediction. \par

\noindent Training data is typically subdivided into three sets: training, validation and test. The training set provides in-sample data - input-output pairs which are exposed to the model during training and used to update its parameters. The validation set approximates out-of-sample data used to adjust model architecture, as well as hyperparameters (meta model parameters which configure its operation). The test set is used to assess the models prediction performance after training is complete. These results establish an objective measure of the model's utility.  \par

\noindent An objective function is used to measure the prediction performance of a model during training, and can compute an error or reward signal. When the objective computes an error, it is called a loss function, which measures the difference between the model's predicted output and the target output (the labels in the training set). As the model parameters are updated during training, the loss value changes and produces a multidimensional loss surface. The training algorithm tries to minimise this loss using a method of optimisation, such as bayesian, evolutionary or first order \unskip ~\citep{hastie2009elements, bishop2006pattern, murphy2012machine}. \par

\noindent A popular method of optimisation is gradient decent, an approach which attempts to descend to the lowest region of the loss surface: the global minimum. Model parameter updates are computed based on partial derivatives of the loss with respect to the individual parameter. The magnitude of the update is dependent on the magnitude of the loss i.e. the bigger the error, the bigger the update. A training algorithm uses such an optimsation method to guide the loss toward the global minimum, but in practice a local minimum is usually the best that can be achieved. This process in turn tries to guide the model toward a representative approximation of the underlying distribution of the data, enabling it to make predictions. \par

\noindent During training, mini-batch optimisation is used along with gradient descent to update model parameters. In mini-batch optimisation, the training set is divided into batches, where model parameters are updated after a mini-batch has been processed. This is in contrast to stochastic gradient descent where parameters are updated per sample, and batch gradient descent where parameters are updated after a run through the entire training set. Mini-batch optimisation is both robust to convergence (not as susceptible to local optima), and compute efficient. \newline
The supervised learning training algorithm is expressed as follows: \par 

\bigskip

\begin{algorithm}[H]
	\SetAlgoLined
	\textbf{Input} 
	Training set \begin{math} D = \{(x_i, y_i)\}_{i=1}^N \end{math}, of input features and output labels\;
	Intialise parameters $w \in \mathbb{R}$, for model \begin{math} M(w) \end{math} // e.g. random normal initialisation \\
	\For{Epoch \begin{math} k,  k \in [1, K]  \end{math}}{
  		\begin{math} S_{batch} \gets sample(D, b) \end{math} // sample minibatch of size \begin{math} b \end{math} \\
	 	\For{(x, y) \begin{math} \in S_{batch} \end{math}}{
     			\begin{math} y' \gets predict(x, \; y') \end{math} // predict label for sample \\
			\begin{math} e \gets y' - y \end{math} // compute loss
     			}
		$ w \gets w - \lambda * \hat{\nabla}_{w} J_{e} (w)$ // partial derivative update of model with respect to cost $e$
	}
	\caption{Supervised Learning}
\end{algorithm} \bigbreak


%********************************** %Second Section  **************************************

\section{Deep learning models}

Deep learning models have become popular for classification and regression tasks, and machine learning tasks in general. The reason for this is that deep models solve a major problem with shallow models, which is to select the best features with which to represent raw input samples \unskip ~\citep{Goodfellow-et-al-2016}. The problem of feature selection had resulted in a methodology used during shallow model development, called feature engineering. It includes techniques such as bucketing, crossing, hashing and embedding \unskip ~\citep{murphy2012machine, Goodfellow-et-al-2016}. These techniques are designed to build feature representations that will result in high classification and regression accuracy. Deep models attempt to discover optimal representations automatically during training, hence their superior performance on a number of machine learning tasks. \par

\noindent Multilayer perceptrons (MLPs) were the early deep learning models implemented as feed-forward neural networks consisting of $N$ layers, applied to an input vector $ x $. These models compute un-normalised scores known as logits, for each of the possible $ M $ outputs, classes and values in classification and regression respectively. They do this by executing a number of steps in an algorithm called the forward pass. Each step is called a layer, where an MLP can consist of a number of layers. A basic MLP consists of three of these layers, an input, hidden and output layer. The MLP computes a linear combinations of input features, then transforms the representation using a nonlinear hidden activation layer, before computing a linear combination of outputs. MLP models can be defined as follows:

\begin{subequations}
	\begin{gather}
		f_0 = x \\
		f_i=\sigma_i(W_if_{i - 1} + b_i) \quad i \in [1, N]
	\end{gather}
\end{subequations}

where $f_0$ is the input layers, $f_i$ is the respective computation layer. Each layer has a particular number,  $m_i$, of neurons. The parameters of a layer consist of a matrix $W_i \in \mathbb{R}^{m_i \times m_{i-1}}$ and bias vector $b_i \in  \mathbb{R}^{m_i}$. Each layer also has a non-linear activation function $\sigma_i$. \par

Loss functions are used to train deep models under a supervised learning paradigm. The error computed by the loss is used in a process called back propagation - the computing of parametric first order partial derivatives, and the adjustment of the parameters in direction and magnitude of their respective derivative. This process is given by:

\begin{subequations}
	\begin{gather}
		\frac{\partial E} {\partial W_n} = \frac{\partial F} {\partial W}(W_n, X_{n-1})\frac{\partial E} {\partial X_n} \\
		\frac{\partial E} {\partial X_{n-1}} = \frac{\partial F} {\partial X}(W_n, X_{n-1})\frac{\partial E} {\partial X_n} 
	\end{gather}
\end{subequations}

where $\frac{\partial F} {\partial W}(W_n, X_{n-1})$ is the Jacobian of $F$ with respect to $W$ evaluated at the point $(W_n, X_{n-1})$, and  $\frac{\partial F} {\partial X}(W_n, X_{n-1})$ is the Jacobian of $F$ with respect to $X$. The Jacobian of a vector function is a matrix containing the partial derivatives of all the outputs with respect to all the inputs. Together the forward pass algorithm and back propagation, allow the automatic discovery of the most meaningful representation to compute the model output. We refer the reader to \unskip ~\citep{Goodfellow-et-al-2016} for a review on loss functions, as well as a more detailed discussion on the forward pass and back propagation. 

%********************************** %Convolutional Networks  **************************************

\subsection{Convolutional Networks}

MLPs suffer from an explosion of parameters  ~\citep{krizhevsky2012imagenet}. In fact when modelling a sample using a regular feed-forward network, we find that the number of model parameters grows exponentially. For example, a feed-forward network with $2$ hidden layers consisting of $512$ and $256$ neurons respectively, an output size of $10$ and an input sample shape of $\left [ \begin{matrix} 200 & 1 \end{matrix} \right] $, then the model ends up having: 


\begin{multline}
		total parameters = (X_D \times L_{D_i} + L_{D_i}) + (L_{D_i} \times L_{D_{i+1}} + L_{D_{i+1}}) \\ 
		+ \dots + (L_{D_{i+N-1}} \times L_{D_{N}} + L_{D_{N}}) \quad i \in [1, N]
\end{multline}

where $X_D$ is the input dimension size, $L_D$ is the number of nodes and $i$ is the layer number, for a total of $236,810$ parameters. Compounding this issue, is the incapability of MLPs to take advantage of structure in data, thus having no way of compensating for distributions of representations of the same conceptual sample. In order to cater for sample representation distribution variance, MLPs have to be larger, so as to have enough learning capacity to be able to hold sufficiently representative feature mappings ~\citep{lecun1998gradient}. \newline
Convolutional neural networks (CNN) have been developed to overcome both the above mentioned problems experienced by MLPs. CNNs make use of the convolutional operation during representation learning. This operation allows CNNs to achieve representation translation and location invariance ~\citep{simonyan2014very}. Translation invariance allows the CNN to build an object representation that is consistent under transformation, for example an image rotation would lead to a consistent final sample representation being generated. Locality invariance allows the model to generate the same representation a sample concept if the concept signal shifts along the dimensions of the representation. These properties allow CNNs to model the true distribution of samples using fewer parameters and smaller datasets. In the following we discuss the CNN forward pass algorithm. \newline
\textbf{Convolutional Layers.} CNNs perform a convolutional operation on a sample using a trainable representation filter. The operation constructs latent features of the sample by generating a feature map representation. Every filter is N-dimensional, for example 2-dimensional, and small spatially (along width and height), but extends through the full depth of an N-dimensional input volume. During the forward pass, each filter is convolved across the width and height of the input volume, where the element-wise dot product is computed between the entries of the filter and the input at any position. A 2-dimensional feature map is generated as a result, that gives the responses of that filter at every spatial position. Every convolutional layer has a set of corresponding filters, and each of them produce separate 2-dimensional feature maps, which are then stacked along the depth-dimension to produce an output volume. The size of the output volume is controlled by the hyperparameters of the convolutional layer: the filter size $F$ defines the width and height of the filters in the layer. Filters always have the same depth as the inputs to the layer. Depth $D$ of the layer defines the number of filters in the layer. Stride $S$ defines the number of entries by which we move the filter when convolving it along the input volume. Padding $P$ refers to the number of 0 entries we add to the input volume along the width and height dimensions. This parameter is useful in that it gives us more control over the desired size of the output volume and is used to ensure that the output volume has the same width and height as the input volume ~\citep{DLIndaba2017}.\newpage
The convolution forward pass is given by:

\begin{equation}
	O_{ij}^{d} = b_d + \sum_{a=0}^{F - 1}\sum_{b=0}^{F - 1}\sum_{c=0}^{I - 1}W_{a,b,c,d}X_{i+a,j+b,c}^{pad}
\end{equation}

where $O$ is the value of the output volume at position $(i,j,d)$, $b$ bias vector of shape $\left [ \begin{matrix} D \end{matrix} \right]$, $W$ is the weight tensor of shape $\left [ \begin{matrix} F & F & I & D \end{matrix} \right]$, $X$ is the padded input volume and $a, b, c$ are the volume dimensions. \newline
\textbf{Pooling Layers.} A pooling layer is used to reduce the spatial size of the representation. They are used to reduce the number of parameters in the network. Pooling layers provide latent features deeper int the network with a larger receptive field - input regions that they look at in order to represent larger sample surfaces. In particular, pooling stride gives deeper latent features much larger receptive fields so that they can effectively combine smaller features together. Pooling layers apply some 2-dimensional aggregation operation (usually a max, but others like average may also be used) to regions of the input volume. A pooling layer has no trainable parameters itself.  \newline
\textbf{Fully Connected Layers.} In order to compute the output of a CNN, a fully connected layer is required to flatten the output volume. This layer performs a tensor operation that computes an output vector $Y \in \mathbb{R}^{m}$. \newline
\textbf{Convolutional Backpropagation.} Computing parameter updates for convolutional filters is follows the same first order differential procedure used in backpropagation for MLPs. Assuming a loss function $L$ and having computed the derivative of this loss up to the output of the convolutional layer, $\frac{\partial L} {\partial O}$, in order to update the parameters the convolutional layer, we require the derivative of $L$ with respect to the weights and biases of the convolutional layer $\frac{\partial L} {\partial W}$ and $\frac{\partial L} {\partial b}$. We also require the derivative with respect to the inputs of the layer $\frac{\partial L} {\partial X}$ in order to propagate the error back to the preceding layers. \newline
The convolution backpropagation algorithm is thus given by:

\begin{subequations}
	\begin{gather}
		\frac{\partial L} {\partial b} = \frac{\partial L} {\partial O}\frac{\partial O} {\partial b} \\
		\frac{\partial L} {\partial W_{a,b,c,d}} = \sum_{i=0}^{O_w - 1}\sum_{j=0}^{O_h - 1}\frac{\partial L} {\partial O_{ij}^{d}}X_{i+a,j+b,c}^{pad} \\
		\frac{\partial L} {\partial X_{m,n,c}^{pad}} = \sum_{i=0}^{O_w - 1}\sum_{j=0}^{O_h - 1}\sum_{d=0}^{D - 1}W_{m-i,n-j,c,d}\frac{\partial L} {\partial O_{ij}^{d}}
	\end{gather}
\end{subequations}

%********************************** %Recurrent Networks **************************************

\subsection{Recurrent Networks}

Not all datasets are IID. Non-IID data presents itself in the form of sequences. Natural language is a good example of this, where sentences are comprised of a sequence of words. If we had to try to predict the next word from a set of sequence words, it may be useful to have memory of what came before. Feedforward networks like MLPs and CNNs do not have such a capability and can only map fixed-size input-data to their output labels. Recurrent Neural Networks (RNN) ~\citep{werbos1988generalization} solve this problem by generalising feedforward models to incorporate sequential dependencies. \bigskip

\textbf{Recurrent Layers.} RNNs build context by generating a representation of a window of data samples. This context is called the state vector, and is used to used in regression tasks such as time-series estimation, as well as classification tasks such as language model word classification to augment input sample representations. RNN layers are composed of cells ~\citep{DLIndaba2018}, in contrast to nodes in MLPs and CNNs. Cells are modular units that take as input a self-generated state (context), and output from previous cells. They generate a new state and output as they process a data sample. An RNN cell can thus be modelled as follows: 

\begin{equation}
	h_t = \sigma(W_{hh}h_{t-1} + W_{xh}x_t + b)
\end{equation}

where $h_t$ the cell output, and used as the state vector for input received as the next sequence entry. $\sigma$ is a nonlinear activation function, $W_{hh}$ are the state vector input parameters, $W_{xh}$ are the input parameters, $x_t$ is the input and $b$ is the bias. $h_t$ could then be used to predict the word for the next sequence entry, which would be given by:

\begin{equation}
	y_t = \sigma(W_{hy}h_{t} + b)
\end{equation}

\textbf{Backpropagation Through Time.} Back-propagation through time (BPTT) is a gradient-based process that computes parameter updates by chaining first order derivative cell outputs through sequence-time. An error signal is computed at the current sequence-time, and the first order derivative is computed against all sequence entries in a specific window, using the relative sequence-time-specific output. That time-specific error is then backprograted through the RNN layer to adjust the cell state and input parameters. A fixed sequence length is decided upfront for updating cell parameters. This is called truncated BPTT and has both training algorithm as well as compute resource advantages. \newpage
Assuming a loss function $L$, the gradient of the loss $E_t$ at time $t$ on $W_{hh}$ is a function of the current hidden state and model predictions $\hat{y_t}$ at time $t$:  

\begin{equation}
	\frac{\partial E_t} {\partial W_{hh}} = \sum_{k=0}^{t}\frac{\partial E_t} {\partial \hat{y_t}}\frac{\partial \hat{y_t}} {\partial h_t}\frac{\partial h_t} {\partial h_k}\frac{\partial h_k} {\partial W_{hh}}
\end{equation}

One problem experienced by RNNs is the multiplicative effect of state vector derivates throughout sequence-time. This problem can be expressed as:

\begin{equation}
	\frac{\partial h_t} {\partial h_k} = \prod_J\frac{\partial h_j} {\partial h_{j-1}}
\end{equation}

for $j$ from $k + 1$ to $t$. This multiplicative operation causes small gradients to become progressively smaller as the sequence window become larger, or become much larger. This is called the vanishing or exploding gradient problem and has lead to the development of RNN variant models such as Long Short-Term Memory ~\citep{hochreiter1997long} and Gated Recurrent Units ~\citep{cho2014learning}.


%********************************** %Third Section  **************************************

\section{Regularisation}

Model over parameterisation can lead to overfitting of in-sample data. This because the degrees of freedom available to estimate a function allow very complex nonlinear functions to be discovered, where these nonlinear functions are not representative of the generative process of the data. In fact, the model ends up modelling noise in the dataset. Overfitting results in poor generalisation across training, validation and testing datasets. This problem forms part of the broader bias/variance trade off, specifically high model variance due to over parameterisation. Deep learning models are particularly sensitive to overfitting given the high number of parameters present within the model. Regularisation techniques are used to improve the generalisation capability of deep models across training and test datasets. \bigskip

\textbf{L1 and L2 Regularisation.} Lasso (L1) ~\citep{tibshirani1996regression} and Ridge (L2) ~\citep{hoerl1970ridge} regression are two common approaches used in regularisation. These approaches prevent overfitting by adding a term to the loss that penalizes the model if it becomes too complex. They ares defined as follows:

\begin{subequations}
	\begin{gather}
		loss_{L1} = loss + \lambda\sum_i\abs{w_i}  \\
		loss_{L2} = loss + \lambda\sum_iw^2
	\end{gather}
\end{subequations}

L1 regularization has the effect of forcing some parameters to shrink to 0, effectively removing them from the model. L2 regularization has the effect of preventing any of the parameters from becoming too large and overpowering the others. \newline
\textbf{Early Stopping.} As deep models are trained using a supervised learning paradigm, the model training accuracy gradually increases. As training continues, the model validation accuracy gradually increases, and then begins decreasing. In this scenario the model has begun overfitting the training data. If the model continues training, the validation accuracy may continue decreasing, and should the model then be tested, the resulting test accuracy may be lower than the training accuracy. Early stopping ~\citep{prechelt1998early} is a technique used to try to prevent overfitting. Once the training and validation accuracies begin to diverge, training continues for a few more epoch, and if the divergence persists, training is terminated. \newline
\textbf{Dropout.} During training, nodes in a deep model can generate redundant latent features that overfit training data. Dropout is a technique that is used to zero out a proportion of neurons in the layer of a deep model ~\citep{srivastava2014dropout}. This has the effect of removing partial dependence between nodes deeper within the network, a form of principal component analysis generating independent latent features. These latent features result in a simpler model representation of the generative process of the data, and therefore allow it to generalise better across datasets. \newline
\textbf{Batch Normalisation.} Deep models are composed of parameterised layers. These parameters are adjusted during training, resulting in a distribution shift of node output within each layer, a problem known as internal covariate shift. This distribution shift makes it difficult for the model to map the generative distribution of the data, and batch normalisation has been proposed as a technique to compensate for this problem ~\citep{ioffe2015batch}. Batch normalisation solves two other problems experienced during deep model training: nonlinearity saturation, and weight initialisation sensitivity, shortening the required training time. \newline
Batch normalisation works by computing the layer-specific moving average estimators of the batch mean and standard deviation. The normalised output is then scaled and shifted before being used as input for the following layer. Each layer's node output is then modified by these statistics before being passed onto the following layer. The training time batch statistics are then aggregated into global population statistics during test time. \newline
Batch normalisation during training time is thus defined as follows:

\begin{subequations}
	\begin{gather}
		\hat{x_i} = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2}} \\
		y_i = \gamma \hat{x_i} + \beta
	\end{gather}
\end{subequations}

Batch normalisation during test time is thus defined as follows:

\begin{subequations}
	\begin{gather}
		\hat{x} = \frac{x - E \left [ x \right ]}{\sqrt{VAR \left [ x \right ]}} \\
		y = \gamma \hat{x} + \beta
	\end{gather}
\end{subequations}

where $x$ is the normalised layer output, and $y$ is the scaled and shifted next layer input. 


%********************************** %Fourth Section  **************************************

\section{Summary}

In this chapter we've discussed: \newline
\textbf{Supervised Learning.} A paradigm used to train ML models where target output is provided with corresponding input. This paradigm helps models build an input-output map the approximates the underlying data distribution. The effectiveness of this mapping is heavily dependent on the input representation of samples, known as features, and can be used to model IID and non-IID datasets. \newline
\textbf{Deep Learning Models.} We can take advantage of automatic representation construction using deep learning models. Deep models differ from their shallow counterparts by computing latent feature representations directly from the data. This has shown to be more effective then manually constructing features representations using techniques such as bucketing, crossing, hashing and embedding. CNNs solve the over parameterisation problem in MLPs, and simultaneously take advantage of the structure present in data samples. RNNs provide context of previous samples using state vectors when modelling sequential data. Both these approaches have been applied successfully in classification and regression tasks. \newline
\textbf{Regularisation}. Deep models can overfit data and as a result poorly generalise across training and test datasets. A number of approaches have been used to address this problem including lasso and ridge regression, early stopping and dropout. Batch normalisation has proved particularly effective at regularisation as it compensates for the covariate shift experienced by deep models during training. In addition to addressing covariate shift, batch normalisation also speeds up training time, and makes deep models less sensitive to weight initialisation.
