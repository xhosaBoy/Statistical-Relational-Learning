%!TEX root = ../thesis.tex
%*******************************************************************************
%*********************************** First Chapter *****************************
%*******************************************************************************

\chapter{Introduction}  %Title of the First Chapter

\ifpdf
     \graphicspath{{Figs/Chapter1/}}
\else
    \graphicspath{{Chapter1/Figs/Vector/}{Chapter1/Figs/}}
\fi


%********************************** %First Section  **************************************

\section{Overview} %Section - 1.1 

\emph{"... as they say, your model is only as good as the data you have. \newline So it all starts with the data ..."} \newline
\indent \indent - Nyalleng Moorosi, \emph{Deep Learning Indaba} (2017) \par

\smallskip

\noindent Artificial intelligence (AI) is the imitation of human-level intelligence by machines. Human beings are capable of extracting information from a passage of text, and transforming that information into a mental model of objects and relationships between those objects. As stated by Nyalleng Moorosi, the model is only as good as the data data available for training, and this is true for both human mental models as well as machine constructed models. \par

\noindent Human models are often confined to a finite domain \unskip~\citep{staab2010handbook}, for example the entertainment industry. The entertainment industry contains objects such as films, actors, actresses, characters and awards. These objects have relationships between them, such as "stared in", "worked along side", "a character in", and "was nominated for". Humans can develop a mental model to easily reason about new relationships between the objects that were not mentioned in a given passage of text. For example the text may say that Chadwick Boseman stared in a movie called Black Panther, and using that information we can reason that Chadwick Boseman "is an" actor. \par

\noindent We use such models to read and understand other passages of text, in conversations with other people, and to help us answer questions. The set of objects and the relationships between them, within a domain, can be thought of as knowledge. Figure 1.1. shows what knowledge about the entertainment industry might look like. \newline

\begin{figure}
   	\centering
    	\includegraphics[width=\textwidth]{Objects_and_the_Relationships_Between_Them}
	\caption{Objects in the entertainment industry, and relationships between them.}
\end{figure}

\subsection{Challenges} 

Reasoning about knowledge expressed in natural language \unskip~\citep{minervini2019differentiable} is simple for humans, but trying to imitate this behaviour on a computer reveals its underlying complexity. \par

\noindent The first task is providing a mechanism for perceiving syntax. Humans recognise text as sequences of words, and words as a sequence of characters. Characters can belong to different writing systems which can be dense (have a few characters used in a large number of combinations), or sparse (have a large number of characters used in fewer combinations) \unskip~\citep{Hua2010}. A computer has to first be able to perceive these characters. \par

\noindent The second task is even more challenging: modelling semantics. Semantics is the meaning of words in language \unskip~\citep{chomsky1955logical}. It allows humans to understand each other in conversation, through written text, and through visual imagery. Semantics allows us to understand that an actor is a thing, and that things can have relationships with other things. Giving computers the capability of semantic understanding is challenging because of the unstructuredness of language. If a person is shown a word in the singular and a word in the plural, for example "film" and "films", a person will most likely understand that there is not much difference in meaning between the two words, while a computer may interpret them as having completely different meanings. Different words can also have similar meanings, for example "film" and "movie". Capturing this similarity once again challenging for machines. And finally the order in which words are seen can provide context, for example the word "star" has completely different meaningsf in "Chadwick Boseman is a movie star" and "Black Panther looked up into the night sky an saw a star". \par

\noindent The third challenging task is organising information in such a way as to be able to reason about the knowledge it conveys. Humans build mental models using default reasoning \unskip~\citep{reiter1980logic}, believing that most objects A have some relationship X with objects B, with a small number of exceptions. For example, we could believe that films (object A) have not won (relationship X) an Oscar (object B). We will believe this is true for any given movie, unless we are familiar enough with movie history to identify the exceptions. Formallly, the default beliefs we hold are facts, and this process of reasoning allows us to infer new facts in the form of plausible relationships between objects. \par

\noindent In order to allow computers to use the same method of reasoning, data has to be structured as facts and stored in a database. Such a database is called a knowledge base (KB) \unskip~\citep{carlson2010toward, angeli2013philosophers}  and is modelled on ontologies. A KB is used to store facts, which encode knowledge about a domain. These facts are modelled as triples of the form (subject, predicate, object), where the subject and object are entities, and the predicate is a relation between two entities. For example, a fact (triple) in the entertainment KB would be Black Panther (subject/entity) is a (predicate/relation) super hero (object/entity). This model is derived from the resource description framework model of the semantic web \unskip~\citep{bizer2009dbpedia}. \par

\noindent The problem with this approach is that facts are either present in the database or they are not, i.e. questions can be answered exactly, or they cannot. There is no measure of plausibility that can be used to accept new potential facts, or discard implausible facts \unskip~\citep{koller2007introduction}. Statistical relational learning (SRL) aims to address this problem by constructing models with measures of uncertainty in plausible facts not contained in a KB \unskip~\citep{koller2007introduction}.\par

\subsection{Encourging progress} 

The task of reasoning about knowledge expressed in natural language is a daunting one, however there has been encourging progress in SRL. The first major milestone was the integration of the bilinear tensor product and neural networks \unskip~\citep{socher2013reasoning}. This approach effectively extended linear tensor factorisation - the scoring of potential relationships between entities - techniques to nonlinear tensor factorisation. The approach simultaneously the use of pre-trained word embeddings to initialise the encoding of entities and relations, instead of using randomly initialised word vectors. In this one approach, both reasoning and semantic representations were extended. \par

\noindent The second time a major milestone was realised was with the use of complex valued embeddings for link prediction \unskip~\citep{trouillon2016complex} - inferring plausible relationships between entities. This approach extended the bilinear tensor product - the multiplicative composition of subject vector, predicate matrix and object vector - by making use of the Hermitian dot product, which is the complex counterpart of the standard dot product between real vectors. It was proposed that complex vectors can effectively capture antisymmetric relations, "is not", while retaining the efficiency benefits of the dot product. This was achieved by using representations with complex embeddings, allowing the model to capture semantic meaning by computing a relational score dependent on the order of the entities in the triple. \par
 
\noindent The previous two milestones relied on shallow models that could scale to large datasets. Up until that point, research in link prediction had focused on minimising the parameterisation of models. Convolutional networks are parameter efficient models, and major progress was again realised with the use of deep convolutional models for link prediction \unskip~\citep{dettmers2018convolutional}. The approach made use of 2D embedding representations which allowed the modelling of a large number of interactions between embeddings. \par

\noindent Despite this progress, we have yet to see this method of knowledge-based reasoning deployed to real-life applications. Alternative methods aiming to solve the Stanford Question Answering Dataset (SQuAD) \unskip~\citep{rajpurkar2016squad}, the General Language Understanding Evaluation (GLUE) benchmarks \unskip~\citep{liu2019roberta}, and the Alexa Prize \unskip~\citep{ram2018conversational}, have seen greater commercial adoption. Applications of link prediction seem to be more focused on the evolution of relational databases, and have so far found utility in laboratory information management systems \unskip~\citep{HARROW20192068}. 

\subsection{Remaining challenges}

Perhaps the lack of adoption in link prediction is due to the current state-of-the-art (SOTA) performance in open-domain question answering: 25.20\% \unskip~\citep{balazevic2019hypernetwork}. A possible explanation for why SOTA prediction accuracy of neural factorisation methods is so low is that Toutanova and Chen \unskip~\citep{toutanova2015observed} realised inverse relation test set leakage from the training set, i.e. when entities appear as subjects in the training set and as objects in the test set, of FB15k, a subset of the Freebase KB with 15,000 entities. Because of this test set leakage, simple rule-based models were able to exploit inverse relations in the test set and achieve SOTA performance. Similarly, current SOTA performance of neural factorisation methods on the less challenging WN18RR dataset is 43.60\%. Dettmers et al. \unskip~\citep{dettmers2018convolutional} created this dataset after discovering a similar test leakage problem in WN18, a subset of the Wordnet KB with 18 relations. We can conclude that SOTA performance on link prediction has to dramatically improve before wide-spread adoption can be realised. 

\subsection{Outline of contributions}

SOTA neural factorisation models \unskip~\citep{balazevic2019hypernetwork, dettmers2018convolutional} introduce relational covariate shift \unskip~\citep{ioffe2015batch} - distortion of relational matrix parameters during training due to simultaneous updates of parameters in the previous layer. The models extend the bilinear model \unskip~\citep{jenatton2012latent}, a multiplicative linear compositional model, by computing subject-predicate transformations using a convolutional operation \unskip~\citep{zeiler2014visualizing}, instead of a dot product. This extension exacerbates covariate shift between the subject and predicate features. We correct for this exacerbated covariate shift by regularising relational filters using batch normalisation. \par

\noindent Leveraging semantic information \unskip~\citep{socher2013reasoning} from pre-trained word embeddings \unskip~\citep{mikolov2013distributed} can provide richer representations for improved inference during reasoning. We integrate pre-trained word embeddings into SOTA neural factorisation model training to compensate for this sparsity in representational data, providing richer context with which to perform reasoning. 

\subsection{Long-term motivations} 

Reasoning about knowledge expressed in natural language is one of the strongest measures of intelligence possessed by humans. This capability allows challenges discourse, debate and dialogue. In order for humans to realise artificial general intelligence (AGI), this skill has to be mastered by machines. Potential applications that can be designed using such technology have profound implications for all aspects of society, challenges including in education (such as tutoring system), health (such as frontline assistance), and science (from applications in astronomy to energy research). Practical implementations of AGI will likely be grounded in natural language and the internet. This thesis aims to contribute implementations that can help in methodology understanding toward the realisation of full AGI.  

\subsection{Short-term motivations}

We note the poor performance in open domain question answering i.e. inferring potential answers from know facts, currently achieved by SOTA neural factorisation models \unskip~\citep{balazevic2019hypernetwork, dettmers2018convolutional}. A sensible implementation of AGI would use heterogeneous information sources from the internet \unskip~\citep{angeli2013philosophers}. Extracting information from the web, constructing or expanding knowledge bases, and then answering questions \unskip~\citep{shalaby2019beyond} is currently the most successful paradigm for open domain question answering. Given that the most successful models using this paradigm have poor performance, there is an opportunity to refine existing techniques in an effort to make them more useful in answering questions in this way. Revolutionising existing methods that try to solve AI tasks by making use of deep learning techniques has seen tremendous progress in fields such as computer vision \unskip~\citep{hudson2018compositional}  and natural language processing \unskip~\citep{peters2018deep}. Neural factorisation approaches should attempt further integration of deep learning methods, with the adoption of practices from such fields applied to training, reasoning and knowledge representation. Given that deep learning itself is loosely modelled after the human mind, it seems sensible to try to bring SRL into closer alignment with methods of reasoning used by humans. 

\subsection{Challenges of this approach} 

In order to answer open-domain questions, the domain in which the knowledge belongs needs to be defined. Facts about that domain then need to be populated. Typically this is achieved by scanning documents written in natural language on the internet \unskip~\citep{fader2011identifying, dong2014knowledge} and populating a knowledge base. Given the heterogeneity of these data sources, it can be difficult to consolidate the information into a central datastore which can then be used to perform inference.  Related to the problem of centralising knowledge, the number of facts within a domain can range from hundreds of thousands, to millions. This poses a sample scarcity or model density problem. In the former, it becomes a challenge to adequately model the probability distributions of plausible relations given the frequency in observations of facts about a particular subject. In the latter, large parameterisation of models is needed to encode knowledge within the domain. \par

\noindent An additional challenge to this method of question answering is the compute resources required to perform inference. For any given question, current methods perform an inference test on every entity within the knowledge base. This is not a challenge for knowledge bases with a small number of entities, for example tens of thousands of entities, however it poses a serious scalability problem for knowledge bases with entities in the millions. This compute problem presents significant opportunity for more scalable inference implementations. Similarly, question answering is modelled as a classification task, where the classification categories are the entities themselves. This means large numbers of target categories are in the millions, whilst small numbers of target categories are in the tens of thousands. This seems like an impractical method of modelling question answering and also presents opportunity for improvement. 


%********************************** %Second Section  *************************************

\section{Related Work} %Section - 1.2 

\subsection{Reasoning about facts} 

\noindent This thesis aims to extend research that aspires to give computers the capability of human-like reasoning \unskip~\citep{bordes2011learning} in open domain question answering \unskip~\citep{hakimov2019evaluating}. Early work in this area focused on learning deterministic logical concepts based on symbolic frameworks \unskip~\citep{hohenecker2017deep} which were used for formal reasoning. Later, attempts to relax formal reasoning and make use of more flexible embedding representations of natural language inspired research in SRL \unskip~\citep{koller2007introduction}. Link prediction is an SRL approach to human-like reasoning that makes use of knowledge bases \unskip~\citep{balazevic2019hypernetwork, dettmers2018convolutional, socher2013reasoning}. \par

\noindent Early approaches to reasoning over facts in KBs were attempted using tensor factorisation \unskip~\citep{nickel2011three} to model entity-relational interactions. Nickel et al. implemented RESCAL, a model that uses the bilinear tensor product to model interactions where entities are expressed as vectors and and the relation between them is represented as a full rank matrix \unskip~\citep{nickel2012factorizing}. The triple is then scored by computing the product of pairwise interactions between latent features of the respective representations \unskip~\citep{nickel2015review} and producing a measure of confidence in the fact. RESCAL uses the values of the relational matrix to model block structure patterns (groups of entities that have similar relationships to other groups), homophily patterns (the tendency of entities to be related to other entities with similar characteristics), and anticorrelations (the tendency of entities with different characteristics to produce antirelations). \par

\noindent Translating the embeddings (TransE) of entities by Bordes et al. \unskip~\citep{bordes2013translating} was another approach tried to model entity-relational interactions. The motivation for this approach was that a lot of KB facts are presented in hierarchies, therefore a translation of the subject by the predication should produce an embedding close to the object. The model translation is thus the natural transformation of the entity and can be used to model hierarchies, along with embedding equivalence with the null transformation. \par

\noindent Yang et al. proposed DistMult \unskip~\citep{yang2014embedding}, a bilinear diagonal model which first transforms the subject and object entities into low-dimensional vector representations, and then applies a bilinear tensor product operation using a relation matrix with only diagonal elements. This approach effectively models a subset of the entity-relational interactions of RESCAL and relies on the entity transformations to generate sufficient semantic representations. The model's bilinear formulation is extremely parameter efficient, but lacks expressiveness. \par

\noindent ComplEx \unskip~\citep{trouillon2016complex} by Trouillon et al. is the extension of DistMult into the complex domain. Entities are represented using complex vectors, and relations are represented using a diagonal matrix with complex entries. Complex valued embeddings extend the entity-relational interactions modelling to include antisymmetric interactions. \par

\noindent All these approaches rely on linear composite triple representations. Linear composite triple representations scale well with large datasets but are limited in their expressiveness. In the following we will discuss nonlinear composite triple representations. \par

\noindent Neural Tensor Networks (NTN) were proposed by Socher and Chen et al. \unskip~\citep{socher2013reasoning}. NTNs approach link prediction by extending the bilinear tensor product in two ways, 1) They model interactions between subject and object entities using recursive neural network (RSNN). ~\citep{socher2012semantic}. This model constructs a representation that is added to the bilinear tensor product, and then 2) resultant representation is then passed through a neural network. The NTN thus expresses richer semantic information by using the RSNN and increases entity-relational interaction expressiveness using a neural triple representation composition. Socher and Chen also examine the use of pre-trained word embeddings on link prediction. They find models trained using word embeddings outperform models trained using random initialisation. \par

\noindent Hohenecker and Lukasiewicz extend NTNs by pre-computing object representations as aggregations of all the triples in which which the object plays a part \unskip~\citep{hohenecker2017deep}. They introduce the new mode as a Relational NTN (RTN). Dong et al. introduce ER-MLP \unskip~\citep{dong2014knowledge, nickel2015review}, a simplification of NTN. This model extends entity-relational interaction model by passing the bilinear tensor product through neural network. ER-MLP also makes use of low dimensional vectors for word embedding representations. \par

\noindent HolE, by Nicel et al., uses a  circular correlation to create compositional representations \unskip~\citep{nickel2016holographic}. HolE relies on concatenation of the subject and object entities, directly modelling the relational score on shared semantic features of the entities. The relational vector is then used to project the representation, before a sigmoidal non-linearity is used to complete the composition and score the relation. This approach is particularly adept at modelling anti-symmetric interactions. HolE introduces the concept of dynamically training embeddings to represent observed relations instead of storing static entity and relation embeddings. \par

\noindent Dettmers et al. introduce ConvE \unskip~\citep{dettmers2018convolutional}. ConvE provides a more expressive model by taking the convolution of triple entities with a 2-dimensional convolutional relational model. The subject vector and the relation vector are each reshaped into a matrix and lengthwise contactinated. A convolution operation is then performed on the matrices where the relation matrix performs the task of the relational filter. The obtained feature maps are then flattened and put through a fully connected layer. Finally, the inner product of the generated representation is taken with all object entity vectors to generate a score for each triple. ConvE  increases the expressiveness by modelling entity-relational interactions  around the entire concatenation line, therefore modelling additional points of interaction between embeddings. \par

\noindent The above modelling approaches form part of latent feature modelling in link prediction. Two other research categories, namely graph modelling and inductive probabilistic logic programming, represent attempts at link prediction. We refer the reader to \unskip~\citep{nickel2015review} for a recent review of approaches. \par

\subsection{Entity and relation representations} 

\noindent Previous work represented entities and relations using randomly initialised, static vector representations. This approach does not allow the sharing of statistical strength between the words describing each entity \unskip~\citep{socher2013reasoning}. Socher and Chen showed that using pre-trained word embeddings instead of randomly initialised word embeddings improves link prediction model performance. This performance gain can be explained by the cooccurrence statistics used to train word embeddings. The semantic and syntactic similarity between words captured by word embeddings helps identify plausible interactions between entities. For example we would expect film and character entities are collocated in pre-trained embeddings spaces. The following describes approaches used to generate such embeddings. \par

\noindent Word2Vec, introduced by Mikolov et al. is an algorithm for training distributed representations of words \unskip~\citep{mikolov2013distributed}. The Skip-Gram model ~\citep{mikolov2013efficient} is one of the modes used by Word2Vec to generate word vectors from very large text vocabularies, in the order of 1.6 billion words. The model essentially attempts to take a centre word, and then tries to predict the probability of seeing a context word either to the right of the centre word, or to the left of the centre word. This context window can be any arbitrary size, for example in the previous example the window size is 1. The Skip-Gram model uses the hierarchical softmax \unskip~\citep{morin2005hierarchical} loss function for classification, a computationally efficient approximation of the full softmax, and employs the subsampling of frequent stop words such as “in”, “the”, and “a”. The continuous bag of words (CBOW) \unskip~\citep{mikolov2013efficient} is an alternative approach to the Skip-Gram model. Here the model tries to predict a target word, given a sequence of context words. The CBOW has the advantage of being faster to train and better representations for more frequent words. \par

\noindent GloVe, by Pennington et al. is another unsupervised learning algorithm for generating word embeddings \unskip~\citep{pennington2014glove}. The GloVe model generates word embeddings using the co-occurrence statistics of words in a text vocabulary. The vocabulary is viewed as a global statistics space, and the model begins with a pass over the entire corpus and generates a co-occurrence matrix of a words against all other words in the corpus. GloVe thus captures both global and local statistics of a corpus, a shortcoming of Word2Vec which only captures local corpus statistics. GloVe then attempts to model the co-occurrence statistics between words using a neural network that takes two words as input and scores the co-occurrence statistic as output. The training algorithm then minimises a logarithmic squared error loss that incorporates both global and local corpus statistics. The distributional representation of word embeddings modelling approach that GloVe employs, allows it to outperform Word2Vec on word analogy, word similarity, and named entity recognition tasks. It is thus the chosen word embedding method used in this dissertation.


%********************************** % Third Section  *************************************

\section{Contributions and Outline} %Section - 1.3

In this dissertation we develop models for reasoning about knowledge expressed in natural language applied to open domain question answering. In particular, we develop compensate for covariate shift introduced in convolutional relational filter modelling. We also incorporate the statistical properties of pre-trained word embeddings into neural composition triple representation models for link prediction. \newline
In \textbf{Chapter 2} we discuss training algorithm approaches based on the supervised learning paradigm. We then discuss deep learning models, specifically in relation to convolutional and recurrent neural network models. We provide the mathematical background describing the models, the primary problems they aim to address, as well as examples of applications. We also discuss reguliarsiation techniques used to address the bias/variance trade off. To this end we explore the dropout ~\citep{srivastava2014dropout} and batch normalisation ~\citep{ioffe2015batch} regularisation techniques.   \newline
In \textbf{Chapter 3} we begin by building on top of the NTN model introduced by (Socher and Chen at al. 2013) ~\citep{socher2013reasoning} and reimplemented in Tensorflow by (Doss et al. 2015) ~\citep{Doss2015}. We analyse the neural compositional strengths of the model, as well as RCNN ~\citep{socher2012semantic} entity representations.  We apply modern stochastic and hyperparameter optimisation deep learning techniques to improve the model training algorithm, including adaptive moment estimation ~\citep{kingma2014adam} and hyperparameter random search ~\citep{bergstra2012random}. We then build on top of the SOTA model in link prediction using a latent feature modelling approach - HypER - introduced by (Bala\v{z}evi\'c et al. 2018) ~\citep{balazevic2019hypernetwork}. We explore the strengths of neural compositional modelling using the hypernetwork architecture ~\citep{ha2016hypernetworks}. We identify the covariate shift introduced by modelling relational filters using a hypernetwork, propose a solution to address this problem, and implement a new model HypER+ with these considerations. We then extend HypER+ to make use of pre-trained word embeddings, taking advantage of semantic statistics modelled using GloVe pre-trained word vectors. \newline
In \textbf{Chapter 4} we introduce the Wordnet ~\citep{miller1995wordnet} and Freebase ~\citep{bollacker2008freebase} link prediction benchmark datasets. We establish baseline accuracy performance metrics using the original NTN Tensorflow model. We then train our proposed new model, and compare our hypothesis against the baseline. We then introduce the WN18 ~\citep{bordes2014semantic} and FB15k ~\citep{bordes2013translating} link prediction benchmark datasets, and link prediction benchmark metrics. We implement HypER+ and compare our hypothesis model against the baseline. Finally we introduce the WN18RR ~\citep{dettmers2018convolutional} and FB15k-237 ~\citep{toutanova2015observed} link prediction benchmark datasets. We extend HypER+ to take advantage of GloVe pre-trained embeddings and compare our results against the established baseline metrics using the original HypER model. \newline
Finally, in \textbf{Chapter 5} we summarise the dissertation and examine problems preventing link prediction from use in a greater number of commercial applications. We suggest potential solutions to remaining challenges, and propose future areas of research.


